# -*- coding: utf-8 -*-
"""
fj_judgment_crawler_colab.py
å¸æ³•é™¢åˆ¤æ±ºæ›¸æŸ¥è©¢è‡ªå‹•çˆ¬èŸ²ï¼ˆColab ç‰ˆ, æ”¯æ´åˆ†é èˆ‡PDF/PNGå¿«ç…§ï¼Œä¸¦ç”¢ç”Ÿæ˜ç´°é åˆ†äº«URLåŒ¯ç¸½ï¼‰
- é©ç”¨ Google Colabï¼Œæ‰€æœ‰è¼¸å‡ºå„²å­˜è‡³ /content
- è‡ªå‹•å®‰è£ Playwright èˆ‡ Chromium
"""

# ====== Colab ç›¸é—œè‡ªå‹•å®‰è£ ======
import sys
import os
import asyncio
import pandas as pd
# å…ˆå®‰è£ä¸­æ–‡å­—å‹ï¼ˆé˜²æ­¢ä¸­æ–‡è®Šæ–¹æ ¼ï¼‰
!apt-get install -y fonts-noto-cjk
!pip install beautifulsoup4
# 1. å®‰è£ playwright
if not os.path.exists('/usr/local/lib/python3.10/dist-packages/playwright'):
    !pip install -U playwright
    !playwright install chromium

# 2. Colabå°ˆç”¨è·¯å¾‘
SAVE_DIR = "/content"
os.makedirs(SAVE_DIR, exist_ok=True)
import time
import traceback
import re
import random
import csv
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from datetime import datetime
try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except:
    ZoneInfo = None

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

# ====== å¸¸æ•¸ ======
BASE_URL = "https://judgment.judicial.gov.tw/FJUD/Default_AD.aspx"

WAIT_BEFORE_CHECKBOX = (0.5, 1.2)
WAIT_AFTER_FORM_INPUT = 1
WAIT_AFTER_CLICK_QUERY = (1, 2)
WAIT_HUMAN_NOISE = (0.5, 1.5)
WAIT_DETAIL_CONTENT = (10, 15)
WAIT_PAGE_JUMP = (3, 6)
WAIT_BUSY_PAGE = 300
WAIT_AFTER_ERROR = 3

# æ¸¬è©¦æ¨¡å¼ä¸Šé™ï¼ˆæ­£å¼è·‘è«‹è¨­å¤§æˆ–ç§»é™¤ï¼‰
TEST_MODE_LIMIT_PAGE = 3
TEST_MODE_LIMIT_PER_PAGE = 2

BUSY_MSGS = ["ç³»çµ±å¿™ç¢Œä¸­","æ“ä½œéæ–¼é »ç¹", "è«‹æŒ‰ã€Œé‡æ–°æ•´ç†ã€", "è«‹ç¨å€™å†è©¦", "ç³»çµ±å¿™ç¢Œä¸­ï¼Œè«‹æŒ‰ã€Œé‡æ–°æ•´ç†ã€ã€‚"]

PAGE_URL_LIST_PATH = os.path.join(SAVE_DIR, "Page_URL_List.txt")
CSV_RESULT_PATH = os.path.join(SAVE_DIR, "judgments.csv")
QUERY_LOG_PATH = os.path.join(SAVE_DIR, "Query_Log.txt")  # å„åƒæ•¸çµ„çš„å½™ç¸½ç´€éŒ„
PDF_SNAPSHOT_PATH = os.path.join(SAVE_DIR, "PDF_Snapshots")
os.makedirs(PDF_SNAPSHOT_PATH, exist_ok=True)
# ====== å°å·¥å…· ======
def print_debug(msg):
    print("[DEBUG]", msg)

# å°‡æ–‡å­—è½‰æˆå®‰å…¨æª”å
def slugify(value: str) -> str:
    banned = '\\/:*?"<>|'
    for ch in banned:
        value = value.replace(ch, "_")
    value = "_".join(value.split())           # ç©ºç™½â†’åº•ç·š
    value = re.sub(r"_+", "_", value).strip("_")
    return value[:80]                          # é¿å…éé•·

# è‹¥æª”åå·²å­˜åœ¨ï¼Œå°±è‡ªå‹•åŠ  _2, _3... é¿å…è¦†è“‹
def uniquify(path: str) -> str:
    base, ext = os.path.splitext(path)
    i, new = 2, path
    while os.path.exists(new):
        new = f"{base}_{i}{ext}"
        i += 1
    return new

# å»ºç«‹ã€Œæœ¬æ¬¡åƒæ•¸å°ˆå±¬çš„ PDF ç›®éŒ„ã€
def build_params_pdf_dir(params: dict) -> str:
    # æŠŠå¸¸ç”¨æ¢ä»¶çµ„æˆå¯è®€ç‰‡æ®µï¼ˆå¤ªé•·æœƒè¢« slugify æˆªæ–·ï¼‰
    date_range = f"{params.get('start_year','')}{params.get('start_month','')}{params.get('start_day','')}" \
                 f"-{params.get('end_year','')}{params.get('end_month','')}{params.get('end_day','')}"
    parts = [
        f"date-{date_range}",
        f"year-{params.get('case_year','')}" if params.get('case_year') else "",
        f"type-{params.get('case_type','')}" if params.get('case_type') else "",
        f"no-{params.get('case_number_start','')}_{params.get('case_number_end','')}" if (params.get('case_number_start') or params.get('case_number_end')) else "",
        f"cause-{params.get('case_cause','')}" if params.get('case_cause') else "",
        f"kw-{params.get('full_text','')}" if params.get('full_text') else "",
    ]
    human = slugify("__".join([p for p in parts if p]) or "params")
    # ç”¨å°åŒ—æ™‚é–“åŠ å€‹æ™‚é–“æˆ³ï¼Œè®“æ¯æ¬¡åŸ·è¡Œä¸äº’ç›¸è¦†è“‹ï¼ˆä¹Ÿæ›´å¥½è¿½è¹¤ï¼‰
    ts = datetime.now(ZoneInfo("Asia/Taipei")).strftime("%Y%m%d_%H%M%S") if ZoneInfo else datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    folder = os.path.join(PDF_SNAPSHOT_PATH, f"{ts}__{human}")
    os.makedirs(folder, exist_ok=True)
    return folder

def now_tpe_str():
    if ZoneInfo:
        t = datetime.now(ZoneInfo("Asia/Taipei"))
    else:
        t = datetime.utcnow()
    return t.strftime("%Y-%m-%d %H:%M:%S")

def ensure_headers():
    # åªåœ¨æª”æ¡ˆä¸å­˜åœ¨æ™‚è£œä¸Šè¡¨é ­
    if not os.path.exists(PAGE_URL_LIST_PATH):
        with open(PAGE_URL_LIST_PATH, 'w', encoding='utf-8') as f:
            f.write('æ¨™é¡Œ\tåˆ†äº«ç¶²å€\n')
    if not os.path.exists(QUERY_LOG_PATH):
        with open(QUERY_LOG_PATH, 'w', encoding='utf-8') as f:
            f.write('å…¨æ–‡å…§å®¹\tè£åˆ¤æ¡ˆç”±\tæŸ¥è©¢æ™‚é–“(å°åŒ—)\tç¸½ç­†æ•¸\n')

def append_query_log(full_text, case_cause, tpe_time_str, total_count):
    with open(QUERY_LOG_PATH, 'a', encoding='utf-8') as f:
        f.write(f"{full_text}\t{case_cause}\t{tpe_time_str}\t{total_count}\n")

def sanitize_filename(name):
    return re.sub(r'[\\/:*?"<>|]+', '_', name)

def clean_url(url):
    # ç§»é™¤ç¶²å€è£¡çš„ &ot=in æˆ– ot=in
    if not url:
        return ""
    parts = urlparse(url)
    qs = parse_qs(parts.query)
    qs.pop('ot', None)
    qs_str = urlencode(qs, doseq=True)
    new_parts = parts._replace(query=qs_str)
    clean = urlunparse(new_parts)
    if clean.endswith('?'):
        clean = clean[:-1]
    return clean

def save_share_url(title, url):
    # å–®ç­†å³æ™‚å¯«å…¥ï¼ˆä¸å†æ¯çµ„æ¸…ç©ºï¼‰
    with open(PAGE_URL_LIST_PATH, 'a', encoding='utf-8') as f:
        f.write(f'{title}\t{url}\n')

# ====== é˜²çˆ¬/äººé¡åŒ–æ“ä½œ ======
async def is_busy_page(page):
    html = await page.content()
    return any(msg in html for msg in BUSY_MSGS)

async def human_noise(page):
    x = random.randint(50, 600)
    y = random.randint(200, 700)
    await page.mouse.move(x, y)
    if random.random() > 0.5:
        await page.mouse.wheel(0, random.randint(200, 1200))
    if random.random() > 0.7:
        await page.mouse.click(random.randint(100, 800), random.randint(200, 800))
    await asyncio.sleep(random.uniform(*WAIT_HUMAN_NOISE))

# ====== å¿«ç…§ ======
async def save_pdf_snapshot(page, filename):
    await page.pdf(path=filename, format="A4", print_background=True)

async def save_png_snapshot(page, filename):
    await page.screenshot(path=filename, full_page=True)

# ====== ç«™å…§è½‰å­˜PDFï¼ˆå„ªå…ˆï¼‰ ======
async def download_via_site_pdf(popup, filename):
    """
    å„ªå…ˆå˜—è©¦é»æ“Šã€Œè½‰å­˜PDFã€æˆ–ç›¸è¿‘æ–‡æ¡ˆçš„æŒ‰éˆ•ï¼›æˆåŠŸå‰‡ä»¥ expect_download ä¸‹è¼‰ä¿å­˜ã€‚
    æ‰¾ä¸åˆ°æˆ–å¤±æ•—å°±ä¸Ÿå‡ºä¾‹å¤–ï¼Œå‘¼å«ç«¯å† fallback ç”¨ page.pdfã€‚
    """
    # å¯èƒ½éœ€è¦å…ˆå±•é–‹é¸å–®
    for menu_sel in ["#moreActions", "button:has-text('æ›´å¤š')", "text=æ›´å¤šåŠŸèƒ½"]:
        menu = popup.locator(menu_sel)
        if await menu.count() > 0:
            try:
                await menu.first.click()
                await asyncio.sleep(0.3)
            except:
                pass

    candidates = [
        "text=è½‰å­˜PDF", "text=åŒ¯å‡ºPDF", "text=å¦å­˜ç‚ºPDF",
        "a:has-text('PDF')", "button:has-text('PDF')",
        "[aria-label*='PDF']",
        "#hlExportPDF", "a[href*='.pdf']", "a[href*='toPDF']"
    ]
    target = None
    for sel in candidates:
        loc = popup.locator(sel)
        if await loc.count() > 0:
            target = loc.first
            break
    if not target:
        raise RuntimeError("æœªæ‰¾åˆ°ç«™å…§ã€è½‰å­˜/åŒ¯å‡ºPDFã€æŒ‰éˆ•")

    async with popup.expect_download() as dl_info:
        await target.click()
    download = await dl_info.value
    await download.save_as(filename)
    return filename

# ====== iframe èˆ‡åˆ—è¡¨ ======
async def get_iframe(page):
    for _ in range(60):
        iframe_elem = await page.query_selector("#iframe-data")
        if iframe_elem:
            src = await iframe_elem.get_attribute("src")
            print_debug(f"iframe src: {src}")
            if src and "qryresultlst.aspx" in src:
                for f in page.frames:
                    if src in f.url or f.name == "iframe-data":
                        print_debug(f"å·²æ­£ç¢ºå–å¾— iframeï¼ŒURL: {f.url}")
                        return f
        await asyncio.sleep(0.5)
    return None

async def get_result_table(iframe):
    # æ‹‰é•·ç­‰å¾…ä¸¦å®¹å¿ä¸­é€” iframe å¤±æ•ˆ
    for _ in range(60):  # æœ€å¤šç­‰ 30 ç§’
        try:
            table = await iframe.query_selector('table#jud.jub-table')
            if table:
                return table
        except:
            pass
        await asyncio.sleep(0.5)
    return None

async def iframe_has_no_data_by_header_or_table(iframe):
    """
    å‚³çµ±å¿«é€Ÿåˆ¤æ–·ï¼šçœ‹ <h3>æŸ¥ç„¡è³‡æ–™</h3> æˆ–æ²’æœ‰æ˜ç´°é€£çµã€‚
    ç›®å‰ä¸»æµç¨‹æ”¹ç”¨ wait_results_stateï¼Œé€™å€‹å‡½å¼ä¿ç•™å‚™ç”¨ã€‚
    """
    try:
        hdr = await iframe.query_selector(".page-header h3")
        if hdr:
            text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
            if "æŸ¥ç„¡è³‡æ–™" in text:
                return True
        table = await get_result_table(iframe)
        if not table:
            return True
        rows = await table.query_selector_all("tbody > tr")
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                return False
        return True
    except:
        return True

async def wait_results_state(iframe, max_wait_sec=20):
    """
    ç­‰å¾…åˆ—è¡¨çµæœã€Œå°±ç·’ã€ï¼Œå›å‚³ï¼š
      - "HAS_DATA"  : æ‰¾åˆ°å¯é»æ“Šçš„æ˜ç´°åˆ—
      - "NO_DATA"   : æ˜ç¢ºé¡¯ç¤ºæŸ¥ç„¡è³‡æ–™ï¼ˆæˆ–æ²’æœ‰è¡¨æ ¼/æ²’æœ‰å¯é»åˆ—ï¼‰
      - "UNKNOWN"   : è¶…æ™‚ä»ç„¡æ³•åˆ¤æ–·
    """
    steps = int(max_wait_sec / 0.5)
    for _ in range(steps):
        try:
            # A) çœ‹æ˜¯å¦ç›´æ¥é¡¯ç¤ºã€ŒæŸ¥ç„¡è³‡æ–™ã€
            hdr = await iframe.query_selector(".page-header h3")
            if hdr:
                text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
                if "æŸ¥ç„¡è³‡æ–™" in text:
                    return "NO_DATA"

            # B) çœ‹æ˜¯å¦å·²æœ‰å¯é»çš„æ˜ç´°åˆ—
            table = await iframe.query_selector('table#jud.jub-table')
            if table:
                rows = await table.query_selector_all("tbody > tr")
                for r in rows:
                    link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
                    if link:
                        return "HAS_DATA"
        except:
            pass
        await asyncio.sleep(0.5)

    # è¶…æ™‚å¾Œä¿éšªåˆ¤æ–·
    try:
        table = await iframe.query_selector('table#jud.jub-table')
        if not table:
            return "NO_DATA"
        rows = await table.query_selector_all("tbody > tr")
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                return "HAS_DATA"
        return "NO_DATA"
    except:
        return "UNKNOWN"

async def get_total_result_count_hint(iframe):
    """
    å„ªå…ˆå¾é é¢æ–‡å­—ä¸­è§£æã€Œå…± X ç­†ã€ï¼›æ‰¾ä¸åˆ°å°±å›å‚³ Noneã€‚
    ï¼ˆç«™æ–¹ â‰¤20 ç­†é€šå¸¸ä¸é¡¯ç¤ºç¸½æ•¸ï¼‰
    """
    html = await iframe.content()
    html = html.replace('\u00a0', ' ')
    m = re.search(r"å…±\s*([0-9,]+)\s*ç­†", html)
    if m:
        return int(m.group(1).replace(",", ""))
    return None

async def count_rows_on_current_page(iframe):
    table = await get_result_table(iframe)
    if not table:
        return 0
    rows = await table.query_selector_all("tbody > tr")
    cnt = 0
    for r in rows:
        link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
        if link:
            cnt += 1
    return cnt

async def get_total_pages(iframe):
    select = await iframe.query_selector('select#ddlPage')
    if not select:
        return 1
    options = await select.query_selector_all('option')
    return len(options) if options else 1

async def select_page_by_index(iframe, page, target_index):
    """
    target_index å¾ 1 é–‹å§‹
    """
    select = await iframe.query_selector('select#ddlPage')
    if not select:
        return False
    options = await select.query_selector_all('option')
    if not options or target_index < 1 or target_index > len(options):
        return False
    opt_val = await options[target_index - 1].get_attribute('value')
    if not opt_val:
        return False
    await select.select_option(value=opt_val)
    # è§¸ç™¼ changeï¼ˆASP.NET æœ‰æ™‚éœ€è¦ï¼‰
    try:
        await iframe.evaluate("""(sel)=>{ sel.dispatchEvent(new Event('change', {bubbles:true})); }""", select)
    except:
        pass
    await asyncio.sleep(random.uniform(*WAIT_PAGE_JUMP))
    await page.wait_for_timeout(1000)
    return True

async def goto_page(page, iframe, cur_page):
    try:
        return await select_page_by_index(iframe, page, cur_page)
    except Exception as e:
        print_debug(f"!!åˆ‡æ›åˆ†é å¤±æ•—: {e}")
        return False

# ====== æŸ¥ç„¡è³‡æ–™ï¼ˆQ003ï¼‰åµæ¸¬ & é€å‡ºæŸ¥è©¢ ======
async def is_no_data_q003(page):
    """
    åµæ¸¬ iframe src ç‚º ErrorPage.aspx?err=Q003ï¼Œæˆ–ä¸»é æ¨™é¡Œé¡¯ç¤ºã€æŸ¥ç„¡è³‡æ–™ã€ã€‚
    """
    iframe_elem = await page.query_selector("#iframe-data")
    if iframe_elem:
        src = (await iframe_elem.get_attribute("src")) or ""
        if "ErrorPage.aspx" in src and "err=Q003" in src:
            return True

    hdr = await page.query_selector(".page-header h3")
    if hdr:
        text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
        if "æŸ¥ç„¡è³‡æ–™" in text:
            return True
    return False

async def submit_and_get_iframe(page):
    """
    é€å‡ºæŸ¥è©¢ä¸¦å˜—è©¦å–å¾— iframe çµæœé ã€‚
    é‡ ErrorPage.aspx?err=Q003 æˆ–é æ¨™ã€æŸ¥ç„¡è³‡æ–™ã€â†’ (None, 'NO_DATA')
    æ­£å¸¸å–å¾—åˆ—è¡¨ iframe â†’ (iframe, 'OK')
    å…¶ä»–å¤±æ•— â†’ (None, 'UNKNOWN')
    """
    print_debug("é€å‡ºæŸ¥è©¢æ¢ä»¶...")
    await page.click("#btnQry")
    await asyncio.sleep(random.uniform(*WAIT_AFTER_CLICK_QUERY))

    for _ in range(60):  # æœ€å¤šç­‰ 30 ç§’
        # å…ˆåˆ¤æ–·æ˜¯å¦ã€æŸ¥ç„¡è³‡æ–™ã€
        if await is_no_data_q003(page):
            print_debug("ğŸ” æŸ¥ç„¡è³‡æ–™ï¼ˆErrorPage Q003 / é æ¨™ï¼‰ï¼ŒçµæŸæœ¬åƒæ•¸çµ„ã€‚")
            return None, "NO_DATA"

        # å˜—è©¦å–å¾—çµæœ iframe
        iframe_elem = await page.query_selector("#iframe-data")
        if iframe_elem:
            src = await iframe_elem.get_attribute("src")
            if src:
                print_debug(f"iframe src: {src}")
                if "qryresultlst.aspx" in src:
                    for f in page.frames:
                        if src in f.url or f.name == "iframe-data":
                            print_debug(f"å·²æ­£ç¢ºå–å¾— iframeï¼ŒURL: {f.url}")
                            return f, "OK"

        await asyncio.sleep(0.5)

    print_debug("æœªæ­£ç¢ºå–å¾— iframeï¼ˆé Q003ï¼‰ï¼Œæµç¨‹çµ‚æ­¢")
    return None, "UNKNOWN"

# ====== åƒæ•¸è¼‰å…¥èˆ‡ CSV å¯«å…¥ ======
def load_parameters_from_csv(filepath):
    df = pd.read_csv(filepath, dtype=str).fillna("")
    params_list = []
    for _, row in df.iterrows():
        params_list.append({
            "case_year": row["case_year"].strip(),
            "case_type": row["case_type"].strip(),
            "case_number_start": row["case_number_start"].strip(),
            "case_number_end": row["case_number_end"].strip(),
            "case_cause": row["case_cause"].strip(),
            "case_judgement": row["case_judgement"].strip(),
            "full_text": row["full_text"].strip(),
            "case_size_min": row["case_size_min"].strip(),
            "case_size_max": row["case_size_max"].strip(),
            "start_year": row["start_year"].strip(),
            "start_month": row["start_month"].zfill(2),
            "start_day": row["start_day"].zfill(2),
            "end_year": row["end_year"].strip(),
            "end_month": row["end_month"].zfill(2),
            "end_day": row["end_day"].zfill(2),
            "case_category": [x.strip() for x in row["case_category"].split(';') if x.strip()]
        })
    return params_list

def save_structured_result(data_dict):
    file_exists = os.path.exists(CSV_RESULT_PATH)
    with open(CSV_RESULT_PATH, 'a', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=list(data_dict.keys()))
        if not file_exists:
            writer.writeheader()
        writer.writerow(data_dict)

# ====== å¡«è¡¨ ======
async def fill_query_form(page, params):
    print_debug("è¼‰å…¥æŸ¥è©¢é é¢...")
    await page.goto(BASE_URL, timeout=60000)
    await asyncio.sleep(2)

    print_debug("å‹¾é¸æ¡ˆä»¶é¡åˆ¥...")
    for v in params["case_category"]:
        cb_selector = f'input[name="jud_sys"][value="{v}"]'
        await page.check(cb_selector)
        await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    print_debug("è¼¸å…¥è£åˆ¤æœŸé–“...")
    await page.fill("#dy1", params["start_year"])
    await page.fill("#dm1", params["start_month"])
    await page.fill("#dd1", params["start_day"])
    await page.fill("#dy2", params["end_year"])
    await page.fill("#dm2", params["end_month"])
    await page.fill("#dd2", params["end_day"])
    await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    print_debug("è¼¸å…¥è£åˆ¤å­—è™Ÿèˆ‡é—œéµå­—...")
    if params["case_year"]:
        await page.fill("#jud_year", params["case_year"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_type"]:
        await page.fill("#jud_case", params["case_type"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_number_start"]:
        await page.fill("#jud_no", params["case_number_start"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_cause"]:
        await page.fill("#jud_title", params["case_cause"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_judgement"]:
        await page.fill("#jud_jmain", params["case_judgement"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["full_text"]:
        await page.fill("#jud_kw", params["full_text"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_size_min"]:
        await page.fill("#KbStart", params["case_size_min"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_size_max"]:
        await page.fill("#KbEnd", params["case_size_max"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    await asyncio.sleep(WAIT_AFTER_FORM_INPUT)

# ====== ä¸»è™•ç†ï¼šåˆ†é  + æ˜ç´° ======
async def process_search_pages(page, iframe, context, params, params_pdf_dir=None):
    """
    å›å‚³:
      processed_count: æœ¬æ¬¡å¯¦éš›è™•ç†çš„æ˜ç´°ç­†æ•¸ï¼ˆå— TEST_MODE é™åˆ¶ï¼‰
      final_total_count: æœ€çµ‚ä¼°ç®—/è§£æåˆ°çš„ç¸½ç­†æ•¸
    """
    # å…ˆå–å¾—ç¸½ç­†æ•¸ hintï¼ˆ>20 ç­†æ™‚æ‰å¯èƒ½æœ‰ï¼‰
    total_count_hint = await get_total_result_count_hint(iframe)

    # è‹¥æ²’æœ‰ hintï¼Œæ¨ç®—ç¸½ç­†æ•¸
    final_total_count = None
    if total_count_hint is not None:
        final_total_count = total_count_hint
    else:
        total_pages = await get_total_pages(iframe)
        if total_pages <= 1:
            # å–®é ï¼ˆé€šå¸¸ â‰¤20 ç­†ï¼‰
            first_page_rows = await count_rows_on_current_page(iframe)
            final_total_count = first_page_rows
        else:
            # å¤šé ä½†æœªé¡¯ç¤ºç¸½æ•¸ï¼šåˆ‡åˆ°æœ€å¾Œä¸€é å»æ•¸
            cur_iframe = iframe
            if await select_page_by_index(iframe, page, total_pages):
                await asyncio.sleep(1.5)
                cur_iframe = await get_iframe(page)
                last_rows = await count_rows_on_current_page(cur_iframe)
                # é è¨­æ¯é ï¼ç¬¬1é çš„å¯é»åˆ—æ•¸ã€‚è‹¥è¦æ›´ç²¾æº–ï¼Œå¯å…ˆé‡ç¬¬1é  row æ•¸ã€‚
                first_page_rows = await select_page_by_index(cur_iframe, page, 1) or await asyncio.sleep(1.0)
                cur_iframe = await get_iframe(page)
                page_size = await count_rows_on_current_page(cur_iframe) or 20
                final_total_count = (total_pages - 1) * page_size + last_rows
                # å›åˆ°ç¬¬ 1 é ç¹¼çºŒ
                await select_page_by_index(cur_iframe, page, 1)
                await asyncio.sleep(1.0)
                iframe = await get_iframe(page)

    processed_count = 0
    max_page = TEST_MODE_LIMIT_PAGE

    for cur_page in range(1, max_page + 1):
        print_debug(f"==é–‹å§‹è™•ç†ç¬¬ {cur_page} é ==")
        if cur_page > 1:
            if not await goto_page(page, iframe, cur_page):
                break
            await asyncio.sleep(2.0)
            iframe = await get_iframe(page)
            if not iframe:
                print_debug("é é¢è·³è½‰å¾Œæ‰¾ä¸åˆ°iframeï¼")
                break

        if "qryresultlst.aspx" not in iframe.url:
            print_debug("iframeå·²è¢«å°å›æŸ¥è©¢é ï¼Œåœæ­¢è™•ç†")
            break

        table = await get_result_table(iframe)
        if not table:
            print_debug("æŸ¥ç„¡çµæœtableï¼Œåœæ­¢ã€‚")
            break
        rows = await table.query_selector_all("tbody > tr")
        print_debug(f"æœ¬é (å«è¡¨é ­)åˆ—æ•¸ï¼š{len(rows)}")

        row_idx = 0
        for row in rows:
            cols = await row.query_selector_all("td")
            if not cols or len(cols) < 4:
                continue
            link = await cols[1].query_selector("a[href*='id='], a[href*='ty=JD'], a[href*='ty=JUDBOOK']")
            if not link:
                continue  # æ’é™¤è¡¨é ­æˆ–éè³‡æ–™åˆ—

            # æ±ºå®šæœ¬é æ˜ç´°çš„è¼¸å‡ºè³‡æ–™å¤¾
            # æ±ºå®šæœ¬é æ˜ç´°çš„è¼¸å‡ºè³‡æ–™å¤¾
            base_dir = params_pdf_dir or PDF_SNAPSHOT_PATH   # æœ‰å‚³ç”¨åƒæ•¸è³‡æ–™å¤¾ï¼›å¦å‰‡é€€å›ç¸½è³‡æ–™å¤¾
            os.makedirs(base_dir, exist_ok=True)             # ä¿éšªèµ·è¦‹

            title = (await cols[1].inner_text()).strip().replace(' ', '')
            date_roc = (await cols[2].inner_text()).strip()
            date_str = date_roc.replace('/', '.')

            safe_title = sanitize_filename(title)
            filename_base = os.path.join(base_dir, f"{date_str}_{safe_title}")

            pdf_path = filename_base + ".pdf"
            png_path = filename_base + ".png"


            if os.path.exists(pdf_path) and os.path.exists(png_path):
                print_debug(f"{pdf_path}ã€{png_path} å·²å­˜åœ¨ï¼Œè·³é")
                continue

            try:
                print_debug(f"æº–å‚™å¦é–‹æ–°åˆ†é é»æ“Šæ˜ç´°é ï¼š{title}")
                # æœ‰äº›ç«™é»æœƒåŒé é–‹å•Ÿï¼Œä¿ç•™ expect_page çš„åŒæ™‚ï¼Œå®¹å¿åŒé å°èˆª
                async with context.expect_page() as popup_info:
                    try:
                        await link.evaluate("a => a.target = '_blank'")
                    except:
                        pass
                    await link.click(button="middle")
                popup = None
                try:
                    popup = await popup_info.value  # æˆåŠŸé–‹æ–°é 
                    page_like = popup
                except:
                    # æ²’æ–°é  â†’ åŒé 
                    page_like = page

                await page_like.bring_to_front()
                await page_like.wait_for_load_state("domcontentloaded", timeout=20000)
                await asyncio.sleep(random.uniform(*WAIT_DETAIL_CONTENT))
                await human_noise(page_like)

                share_link = clean_url(page_like.url)
                share_title = f"{date_str}_{title}"
                save_share_url(share_title, share_link or "")

                # å…ˆå˜—è©¦ç«™å…§è½‰å­˜ PDFï¼›å¤±æ•—å†ç”¨å‚™æ´
                try:
                    await download_via_site_pdf(page_like, pdf_path)
                    print_debug(f"å·²ç”¨ã€ç«™å…§è½‰å­˜PDFã€ä¸‹è¼‰ï¼š{pdf_path}")
                except Exception as e_download:
                    print_debug(f"ç«™å…§è½‰å­˜PDFå¤±æ•—ï¼Œæ”¹ç”¨æ‰“å°PDFå‚™æ´ï¼š{e_download}")
                    await save_pdf_snapshot(page_like, pdf_path)
                    print_debug(f"å·²å­˜PDF(å‚™æ´)ï¼š{pdf_path}")

                await save_png_snapshot(page_like, png_path)
                print_debug(f"å·²æˆªåœ–ï¼š{png_path}")

                # ====== è§£æçµæ§‹åŒ–è³‡æ–™ ======
                try:
                    html = await page_like.content()
                    soup = BeautifulSoup(html, "html.parser")

                    def get_value_by_label(label):
                        for th, td in zip(soup.select("div.col-th"), soup.select("div.col-td")):
                            th_text = th.get_text(strip=True)
                            if label in th_text:
                                return td.get_text(strip=True)
                        return ""

                    def extract_party_by_prefix(soup, prefix):
                        prefix_clean = prefix.replace(' ', '').replace('\u00a0','')
                        pattern = re.compile(rf"^{prefix_clean}")
                        judgment_start_markers = ["ä¸Š", "ä¸Šåˆ—"]
                        buffer = []
                        collecting = False

                        for div in soup.find_all("div"):
                            text_raw = div.get_text(" ", strip=True)
                            text = text_raw.replace('\u00a0', '').replace(' ', '')
                            if pattern.match(text):
                                collecting = True
                                cleaned = re.sub(pattern, '', text).strip()
                                if cleaned:
                                    buffer.append(cleaned)
                            elif collecting:
                                if any(other in text for other in ["è²è«‹äºº", "æ³•å®šä»£ç†äºº", "ç›¸å°äºº", "ä»£ç†äºº", "ä¸Šè¨´äºº", "æŠ—å‘Šäºº", "é¸ä»»è¾¯è­·äºº", "å†æŠ—å‘Šäºº"] if other != prefix):
                                    break
                                if any(marker in text_raw for marker in judgment_start_markers):
                                    break
                                if not text_raw.strip():
                                    continue
                                cleaned = re.sub(rf"^{prefix}\\s*", '', text_raw.strip())
                                if cleaned != prefix:
                                    buffer.append(cleaned)
                        return ','.join(buffer).strip()

                    title_text = get_value_by_label("è£åˆ¤å­—è™Ÿ")
                    date_text_raw = get_value_by_label("è£åˆ¤æ—¥æœŸ")
                    date_text = re.sub(
                        r"æ°‘åœ‹\s*(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥",
                        lambda m: f"{int(m.group(1))}/{int(m.group(2)):02}/{int(m.group(3)):02}",
                        date_text_raw
                    )
                    cause_text = get_value_by_label("è£åˆ¤æ¡ˆç”±")
                    applicant = extract_party_by_prefix(soup, "è²è«‹äºº")
                    agent = extract_party_by_prefix(soup, "æ³•å®šä»£ç†äºº")
                    respondent = extract_party_by_prefix(soup, "ç›¸å°äºº")

                    pattern = (r'(?P<æ³•é™¢>.+?)\s*(?P<å¹´åº¦>\d{2,3})\s*å¹´åº¦(?P<å­—>\S+?)å­—ç¬¬\s*(?P<å­—è™Ÿ>\d+)\s*è™Ÿ(?P<åˆ‘æ°‘>(æ°‘äº‹|åˆ‘äº‹))?\s*(?P<è£å®šé¡å‹>è£å®š|åˆ¤æ±º|è£åˆ¤|å‘½ä»¤|è£åˆ¤|å…¶ä»–)?')
                    match = re.match(pattern, title_text or "")
                    court = match.group("æ³•é™¢") if match else ""
                    year = match.group("å¹´åº¦") if match else ""
                    case_word = match.group("å­—") if match else ""
                    case_number = match.group("å­—è™Ÿ") if match else ""
                    case_type = match.group("åˆ‘æ°‘") if match and match.group("åˆ‘æ°‘") else ""
                    judgment_type = match.group("è£å®šé¡å‹") if match and match.group("è£å®šé¡å‹") else ""

                    result = {
                        "è£åˆ¤å­—è™Ÿ": title_text,
                        "è£åˆ¤å­—è™Ÿ_æ³•é™¢": court,
                        "è£åˆ¤å­—è™Ÿ_å¹´åº¦": year,
                        "è£åˆ¤å­—è™Ÿ_å­—": case_word,
                        "è£åˆ¤å­—è™Ÿ_å­—è™Ÿ": case_number,
                        "è£åˆ¤å­—è™Ÿ_åˆ‘äº‹æ°‘äº‹": case_type,
                        "è£åˆ¤å­—è™Ÿ_è£å®š": judgment_type,
                        "è£åˆ¤æ—¥æœŸ": date_text,
                        "è£åˆ¤æ¡ˆç”±": cause_text,
                        "è²è«‹äºº": applicant,
                        "æ³•å®šä»£ç†äºº": agent,
                        "ç›¸å°äºº": respondent,
                        "æ˜ç´°ç¶²å€": share_link,
                    }
                    save_structured_result(result)
                    print_debug(f"âœ… å·²å„²å­˜çµæ§‹åŒ–è³‡æ–™ï¼š{title_text}")
                except Exception as e:
                    print_debug(f"!!è§£æçµæ§‹åŒ–è³‡æ–™å¤±æ•—: {e}")

                if await is_busy_page(page_like):
                    print_debug("!!! åµæ¸¬åˆ°é˜²çˆ¬èŸ²æµé‡é™åˆ¶ï¼Œæš«åœ 5 åˆ†é˜å†ç¹¼çºŒ...")
                    if popup:
                        await page_like.close()
                    await asyncio.sleep(WAIT_BUSY_PAGE)
                    continue

                if popup:
                    await page_like.close()
                row_idx += 1
                processed_count += 1

                if row_idx >= TEST_MODE_LIMIT_PER_PAGE:
                    print_debug(f"å·²é”æœ¬é æ¸¬è©¦ä¸Šé™({TEST_MODE_LIMIT_PER_PAGE}ç­†)ï¼Œé€²å…¥ä¸‹ä¸€é ")
                    break
            except Exception as e:
                print_debug(f"!!ä¸‹è¼‰æ˜ç´°é å¤±æ•—: {e}")
                try:
                    # è‹¥å·²ç”¢ç”Ÿ page_likeï¼Œå­˜éŒ¯èª¤å¿«ç…§
                    if 'page_like' in locals() and page_like:
                        err_base = os.path.join(base_dir, f"error_{date_str}_{safe_title}")
                        await save_png_snapshot(page_like, err_base + ".png")
                        await save_pdf_snapshot(page_like, err_base + ".pdf")
                        # ä¸æ˜¯æ‰€æœ‰æƒ…æ³éƒ½èƒ½ closeï¼ˆåŒé å°èˆªï¼‰ï¼Œå¿½ç•¥éŒ¯èª¤
                        try:
                            await page_like.close()
                        except:
                            pass
                    else:
                        print_debug("page_like å°šæœªé–‹å•Ÿï¼Œç„¡æ³•æˆªåœ–")
                    await asyncio.sleep(WAIT_AFTER_ERROR)
                except:
                    pass
                continue

    print_debug("æ¸¬è©¦ä»»å‹™å®Œæˆã€‚")
    return processed_count, (final_total_count if final_total_count is not None else processed_count)

# ====== å–®çµ„åƒæ•¸æµç¨‹ ======
async def run_scraper_with_params(params):
    async with async_playwright() as p:
        # ä¸‹è¼‰ä¸€å®šè¦é–‹ accept_downloads
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = await browser.new_context(accept_downloads=True)
        page = await context.new_page()

        try:

          # [ADD] ç‚ºé€™ä¸€çµ„åƒæ•¸å»ºç«‹å°ˆå±¬ PDF ç›®éŒ„
            params_pdf_dir = build_params_pdf_dir(params)

            await fill_query_form(page, params)
            iframe, status = await submit_and_get_iframe(page)
            if status == "NO_DATA":
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return
            if not iframe:
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return

            # â˜… é‡è¦ï¼šé¿å…éæ—©åˆ¤æ–·ï¼Œå…ˆç­‰çµæœå°±ç·’
            state = await wait_results_state(iframe, max_wait_sec=20)
            print_debug(f"[çµæœç‹€æ…‹æª¢æŸ¥] {state}")
            if state == "NO_DATA":
                print_debug("ğŸ” æŸ¥è©¢çµæœç‚ºã€æŸ¥ç„¡è³‡æ–™ã€ï¼ŒçµæŸæœ¬åƒæ•¸çµ„ã€‚")
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return
            elif state == "UNKNOWN":
                print_debug("âš ï¸ çµæœç‹€æ…‹æœªçŸ¥ï¼ˆé€¾æ™‚æˆ– DOM çµæ§‹è®Šå‹•ï¼‰ï¼Œå…ˆè¦–ç‚º 0 ç­†ã€‚")
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return

            processed_count, final_total_count = await process_search_pages(page, iframe, context, params, params_pdf_dir)

            # å¯«å…¥æŸ¥è©¢å½™ç¸½ç´€éŒ„
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), final_total_count)

        except PlaywrightTimeoutError as e:
            print_debug(f"Timeoutç™¼ç”Ÿ: {e}")
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
        except Exception as e:
            print_debug("ä¸»æµç¨‹å‡ºç¾éŒ¯èª¤: " + traceback.format_exc())
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
        finally:
            print_debug("ç¨‹å¼çµæŸ")
            if browser:
                await browser.close()

# ====== æ‰¹æ¬¡æµç¨‹ ======
async def run_all_tasks():
    """
    å¾ CSV æª”æ¡ˆä¸­è®€å–æ‰€æœ‰æŸ¥è©¢åƒæ•¸çµ„ï¼Œä¾åºå‘¼å«çˆ¬èŸ²ä¸»æµç¨‹ã€‚
    """
    # ä¸€æ¬¡æ€§åˆå§‹åŒ–ï¼ˆä¸å†æ¯çµ„æ¸…ç©ºï¼‰
    ensure_headers()

    # åƒæ•¸æª”è·¯å¾‘æ›´ç©©å¥ï¼šå„ªå…ˆ params(name).csvï¼Œæ‰¾ä¸åˆ°ç”¨ params.csv
    preferred = os.path.join(SAVE_DIR, "params(name).csv")
    fallback = os.path.join(SAVE_DIR, "params.csv")
    if os.path.exists(preferred):
        csv_path = preferred
    elif os.path.exists(fallback):
        csv_path = fallback
    else:
        print("âš ï¸ éŒ¯èª¤ï¼šæœªæ‰¾åˆ° params(name).csv æˆ– params.csvï¼Œè«‹ä¸Šå‚³å¾Œé‡æ–°åŸ·è¡Œã€‚")
        from google.colab import files
        files.upload()
        if os.path.exists(preferred):
            csv_path = preferred
        elif os.path.exists(fallback):
            csv_path = fallback
        else:
            print("âŒ ä¸Šå‚³å¤±æ•—æˆ–ä»æœªæ‰¾åˆ°æª”æ¡ˆï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
            return

    params_list = load_parameters_from_csv(csv_path)
    for i, params in enumerate(params_list, 1):
        print_debug(f"========== é–‹å§‹ç¬¬ {i} çµ„åƒæ•¸ ==========")
        await run_scraper_with_params(params)

# ====== å…¥å£ ======
if __name__ == "__main__":
    await run_all_tasks()

