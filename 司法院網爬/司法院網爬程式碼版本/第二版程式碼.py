# -*- coding: utf-8 -*-
"""
fj_judgment_crawler_colab.py
å¸æ³•é™¢åˆ¤æ±ºæ›¸æŸ¥è©¢è‡ªå‹•çˆ¬èŸ²ï¼ˆColab ç‰ˆ, æ”¯æ´åˆ†é èˆ‡PDF/PNGå¿«ç…§ï¼Œä¸¦ç”¢ç”Ÿæ˜ç´°é åˆ†äº«URLåŒ¯ç¸½ï¼‰
- é©ç”¨ Google Colabï¼Œæ‰€æœ‰è¼¸å‡ºå„²å­˜è‡³ /content
- è‡ªå‹•å®‰è£ Playwright èˆ‡ Chromium
"""

# ====== Colab ç›¸é—œè‡ªå‹•å®‰è£ ======
import sys
import os
import asyncio
import pandas as pd
# å…ˆå®‰è£ä¸­æ–‡å­—å‹ï¼ˆé˜²æ­¢ä¸­æ–‡è®Šæ–¹æ ¼ï¼‰
!apt-get install -y fonts-noto-cjk
!pip install beautifulsoup4
# 1. å®‰è£ playwright
if not os.path.exists('/usr/local/lib/python3.10/dist-packages/playwright'):
    !pip install -U playwright
    !playwright install chromium

# 2. Colabå°ˆç”¨è·¯å¾‘
SAVE_DIR = "/content"
os.makedirs(SAVE_DIR, exist_ok=True)
import time
import traceback
import re
import random
import csv
import json
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from datetime import datetime
try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except:
    ZoneInfo = None

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

# ====== å¸¸æ•¸ ======
BASE_URL = "https://judgment.judicial.gov.tw/FJUD/Default_AD.aspx"

WAIT_BEFORE_CHECKBOX = (0.5, 1.2)
WAIT_AFTER_FORM_INPUT = 1
WAIT_AFTER_CLICK_QUERY = (1, 2)
WAIT_HUMAN_NOISE = (0.5, 1.5)
WAIT_DETAIL_CONTENT = (10, 15)
WAIT_PAGE_JUMP = (3, 6)
WAIT_BUSY_PAGE = 300
WAIT_AFTER_ERROR = 3

# æ¸¬è©¦æ¨¡å¼ä¸Šé™ï¼ˆæ­£å¼è·‘è«‹è¨­å¤§æˆ–ç§»é™¤ï¼‰
TEST_MODE_LIMIT_PAGE = 3
TEST_MODE_LIMIT_PER_PAGE = 2

BUSY_MSGS = ["ç³»çµ±å¿™ç¢Œä¸­","æ“ä½œéæ–¼é »ç¹", "è«‹æŒ‰ã€Œé‡æ–°æ•´ç†ã€", "è«‹ç¨å€™å†è©¦", "ç³»çµ±å¿™ç¢Œä¸­ï¼Œè«‹æŒ‰ã€Œé‡æ–°æ•´ç†ã€ã€‚"]

PAGE_URL_LIST_PATH = os.path.join(SAVE_DIR, "Page_URL_List.txt")
CSV_RESULT_PATH = os.path.join(SAVE_DIR, "judgments.csv")
QUERY_LOG_PATH = os.path.join(SAVE_DIR, "Query_Log.txt")  # å„åƒæ•¸çµ„çš„å½™ç¸½ç´€éŒ„
PDF_SNAPSHOT_PATH = os.path.join(SAVE_DIR, "PDF_Snapshots")
os.makedirs(PDF_SNAPSHOT_PATH, exist_ok=True)

# ====== å››ç¨®è¼¸å‡ºè·¯å¾‘ï¼ˆæ–°å¢ï¼‰ ======
JSONL_PATH = os.path.join(SAVE_DIR, "judgments.jsonl")                 # å…¨æ–‡ JSONLï¼ˆé€ç­†è¿½åŠ ï¼‰
MGMT_CSV_PATH = os.path.join(SAVE_DIR, "mgmt_requirements.csv")        # æ¥­ç®¡éœ€æ±‚æª”ï¼ˆçµæ§‹åŒ–æ‘˜è¦ï¼‰
DB_CSV_PATH   = os.path.join(SAVE_DIR, "db_records.csv")               # è³‡æ–™åº«æª”ï¼ˆæ­£è¦åŒ–å‰çš„æ‰å¹³è¡¨ï¼‰

# ====== çµ±ä¸€æµç¨‹åŒ– LOGï¼ˆæ–°å¢ï¼‰ ======
def log_stage(tag: str, msg: str, **kv):
    tail = " ".join([f"{k}={v}" for k, v in kv.items()]) if kv else ""
    print(f"LOG-{tag} | {msg}" + (f" | {tail}" if tail else ""))

# ====== å°å·¥å…· ======
def print_debug(msg):
    print("[DEBUG]", msg)

# å°‡æ–‡å­—è½‰æˆå®‰å…¨æª”å
def slugify(value: str) -> str:
    banned = '\\/:*?"<>|'
    for ch in banned:
        value = value.replace(ch, "_")
    value = "_".join(value.split())           # ç©ºç™½â†’åº•ç·š
    value = re.sub(r"_+", "_", value).strip("_")
    return value[:80]                          # é¿å…éé•·

# è‹¥æª”åå·²å­˜åœ¨ï¼Œå°±è‡ªå‹•åŠ  _2, _3... é¿å…è¦†è“‹
def uniquify(path: str) -> str:
    base, ext = os.path.splitext(path)
    i, new = 2, path
    while os.path.exists(new):
        new = f"{base}_{i}{ext}"
        i += 1
    return new

# å»ºç«‹ã€Œæœ¬æ¬¡åƒæ•¸å°ˆå±¬çš„ PDF ç›®éŒ„ã€
def build_params_pdf_dir(params: dict) -> str:
    # æŠŠå¸¸ç”¨æ¢ä»¶çµ„æˆå¯è®€ç‰‡æ®µï¼ˆå¤ªé•·æœƒè¢« slugify æˆªæ–·ï¼‰
    date_range = f"{params.get('start_year','')}{params.get('start_month','')}{params.get('start_day','')}" \
                 f"-{params.get('end_year','')}{params.get('end_month','')}{params.get('end_day','')}"
    parts = [
        f"date-{date_range}",
        f"year-{params.get('case_year','')}" if params.get('case_year') else "",
        f"type-{params.get('case_type','')}" if params.get('case_type') else "",
        f"no-{params.get('case_number_start','')}_{params.get('case_number_end','')}" if (params.get('case_number_start') or params.get('case_number_end')) else "",
        f"cause-{params.get('case_cause','')}" if params.get('case_cause') else "",
        f"kw-{params.get('full_text','')}" if params.get('full_text') else "",
    ]
    human = slugify("__".join([p for p in parts if p]) or "params")
    # ç”¨å°åŒ—æ™‚é–“åŠ å€‹æ™‚é–“æˆ³ï¼Œè®“æ¯æ¬¡åŸ·è¡Œä¸äº’ç›¸è¦†è“‹ï¼ˆä¹Ÿæ›´å¥½è¿½è¹¤ï¼‰
    ts = datetime.now(ZoneInfo("Asia/Taipei")).strftime("%Y%m%d_%H%M%S") if ZoneInfo else datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    folder = os.path.join(PDF_SNAPSHOT_PATH, f"{ts}__{human}")
    os.makedirs(folder, exist_ok=True)
    return folder

def now_tpe_str():
    if ZoneInfo:
        t = datetime.now(ZoneInfo("Asia/Taipei"))
    else:
        t = datetime.utcnow()
    return t.strftime("%Y-%m-%d %H:%M:%S")

def ensure_headers():
    # åªåœ¨æª”æ¡ˆä¸å­˜åœ¨æ™‚è£œä¸Šè¡¨é ­
    if not os.path.exists(PAGE_URL_LIST_PATH):
        with open(PAGE_URL_LIST_PATH, 'w', encoding='utf-8') as f:
            f.write('æ¨™é¡Œ\tåˆ†äº«ç¶²å€\n')
    if not os.path.exists(QUERY_LOG_PATH):
        with open(QUERY_LOG_PATH, 'w', encoding='utf-8') as f:
            f.write('å…¨æ–‡å…§å®¹\tè£åˆ¤æ¡ˆç”±\tæŸ¥è©¢æ™‚é–“(å°åŒ—)\tç¸½ç­†æ•¸\n')

def append_query_log(full_text, case_cause, tpe_time_str, total_count):
    with open(QUERY_LOG_PATH, 'a', encoding='utf-8') as f:
        f.write(f"{full_text}\t{case_cause}\t{tpe_time_str}\t{total_count}\n")

def sanitize_filename(name):
    return re.sub(r'[\\/:*?"<>|]+', '_', name)

def clean_url(url):
    # ç§»é™¤ç¶²å€è£¡çš„ &ot=in æˆ– ot=in
    if not url:
        return ""
    parts = urlparse(url)
    qs = parse_qs(parts.query)
    qs.pop('ot', None)
    qs_str = urlencode(qs, doseq=True)
    new_parts = parts._replace(query=qs_str)
    clean = urlunparse(new_parts)
    if clean.endswith('?'):
        clean = clean[:-1]
    return clean

def save_share_url(title, url):
    # å–®ç­†å³æ™‚å¯«å…¥ï¼ˆä¸å†æ¯çµ„æ¸…ç©ºï¼‰
    with open(PAGE_URL_LIST_PATH, 'a', encoding='utf-8') as f:
        f.write(f'{title}\t{url}\n')

# ====== é˜²çˆ¬/äººé¡åŒ–æ“ä½œ ======
async def is_busy_page(page):
    html = await page.content()
    return any(msg in html for msg in BUSY_MSGS)

async def human_noise(page):
    x = random.randint(50, 600)
    y = random.randint(200, 700)
    await page.mouse.move(x, y)
    if random.random() > 0.5:
        await page.mouse.wheel(0, random.randint(200, 1200))
    if random.random() > 0.7:
        await page.mouse.click(random.randint(100, 800), random.randint(200, 800))
    await asyncio.sleep(random.uniform(*WAIT_HUMAN_NOISE))

# ====== å¿«ç…§ ======
async def save_pdf_snapshot(page, filename):
    await page.pdf(path=filename, format="A4", print_background=True)

async def save_png_snapshot(page, filename):
    await page.screenshot(path=filename, full_page=True)

# ====== ç«™å…§è½‰å­˜PDFï¼ˆå„ªå…ˆï¼‰ ======
async def download_via_site_pdf(popup, filename):
    """
    å„ªå…ˆå˜—è©¦é»æ“Šã€Œè½‰å­˜PDFã€æˆ–ç›¸è¿‘æ–‡æ¡ˆçš„æŒ‰éˆ•ï¼›æˆåŠŸå‰‡ä»¥ expect_download ä¸‹è¼‰ä¿å­˜ã€‚
    æ‰¾ä¸åˆ°æˆ–å¤±æ•—å°±ä¸Ÿå‡ºä¾‹å¤–ï¼Œå‘¼å«ç«¯å† fallback ç”¨ page.pdfã€‚
    """
    # å¯èƒ½éœ€è¦å…ˆå±•é–‹é¸å–®
    for menu_sel in ["#moreActions", "button:has-text('æ›´å¤š')", "text=æ›´å¤šåŠŸèƒ½"]:
        menu = popup.locator(menu_sel)
        if await menu.count() > 0:
            try:
                await menu.first.click()
                await asyncio.sleep(0.3)
            except:
                pass

    candidates = [
        "text=è½‰å­˜PDF", "text=åŒ¯å‡ºPDF", "text=å¦å­˜ç‚ºPDF",
        "a:has-text('PDF')", "button:has-text('PDF')",
        "[aria-label*='PDF']",
        "#hlExportPDF", "a[href*='.pdf']", "a[href*='toPDF']"
    ]
    target = None
    for sel in candidates:
        loc = popup.locator(sel)
        if await loc.count() > 0:
            target = loc.first
            break
    if not target:
        raise RuntimeError("æœªæ‰¾åˆ°ç«™å…§ã€è½‰å­˜/åŒ¯å‡ºPDFã€æŒ‰éˆ•")

    async with popup.expect_download() as dl_info:
        await target.click()
    download = await dl_info.value
    await download.save_as(filename)
    return filename

# ====== iframe èˆ‡åˆ—è¡¨ ======
async def get_iframe(page):
    """
    ç”¨é€”ï¼šåœ¨å¤–å±¤ Page ä¸­å°‹æ‰¾ä¸¦å›å‚³ã€ŒæŸ¥è©¢çµæœæ¸…å–®ã€æ‰€ä½¿ç”¨çš„ iframe ç‰©ä»¶ã€‚
    åˆ¤æ–·æ–¹å¼ï¼šæŠ“åˆ° <iframe id="iframe-data">ï¼Œå…¶ src éœ€åŒ…å« 'qryresultlst.aspx'ï¼ˆæ¸…å–®é ï¼‰ã€‚
    å›å‚³ï¼šPlaywright Frameï¼›è‹¥åœ¨ç´„ 30 ç§’å…§æ‰¾ä¸åˆ°å‰‡å›å‚³ Noneã€‚
    """
    for _ in range(60):
        iframe_elem = await page.query_selector("#iframe-data")
        if iframe_elem:
            src = await iframe_elem.get_attribute("src")
            print_debug(f"iframe src: {src}")
            if src and "qryresultlst.aspx" in src:
                for f in page.frames:
                    if src in f.url or f.name == "iframe-data":
                        print_debug(f"å·²æ­£ç¢ºå–å¾— iframeï¼ŒURL: {f.url}")
                        return f
        await asyncio.sleep(0.5)
    return None

async def get_result_table(iframe):
    """
    ç”¨é€”ï¼šåœ¨çµæœ iframe ä¸­å°‹æ‰¾æ¸…å–®è¡¨æ ¼ <table id="jud" class="jub-table"> ä¸¦å›å‚³å…ƒç´ æ§åˆ¶æŸ„ã€‚
    å›å‚³ï¼šElementHandle æˆ– Noneï¼ˆåœ¨ç­‰å¾…æœŸé™å…§æ‰¾ä¸åˆ°æ™‚ï¼‰ã€‚
    èªªæ˜ï¼šä»¥ 0.5 ç§’è¼ªè©¢ä¸€æ¬¡ã€æœ€å¤šç´„ 30 ç§’ï¼Œä¸¦å®¹å¿ iframe åœ¨è¼‰å…¥éç¨‹ä¸­çŸ­æš«å¤±æ•ˆã€‚
    """
    for _ in range(60):  # æœ€å¤šç­‰ 30 ç§’
        try:
            table = await iframe.query_selector('table#jud.jub-table')
            if table:
                return table
        except:
            pass
        await asyncio.sleep(0.5)
    return None

async def iframe_has_no_data_by_header_or_table(iframe):
    """
    å‚³çµ±å¿«é€Ÿåˆ¤æ–·ï¼šçœ‹ <h3>æŸ¥ç„¡è³‡æ–™</h3> æˆ–æ²’æœ‰æ˜ç´°é€£çµã€‚
    ç›®å‰ä¸»æµç¨‹æ”¹ç”¨ wait_results_stateï¼Œé€™å€‹å‡½å¼ä¿ç•™å‚™ç”¨ã€‚
    """
    try:
        hdr = await iframe.query_selector(".page-header h3")
        if hdr:
            text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
            if "æŸ¥ç„¡è³‡æ–™" in text:
                return True
        table = await get_result_table(iframe)
        if not table:
            return True
        rows = await table.query_selector_all("tbody > tr")
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                return False
        return True
    except:
        return True

async def wait_results_state(iframe, max_wait_sec=20):
    """
    ç­‰å¾…åˆ—è¡¨çµæœã€Œå°±ç·’ã€ï¼Œå›å‚³ï¼š
      - "HAS_DATA"  : æ‰¾åˆ°å¯é»æ“Šçš„æ˜ç´°åˆ—
      - "NO_DATA"   : æ˜ç¢ºé¡¯ç¤ºæŸ¥ç„¡è³‡æ–™ï¼ˆæˆ–æ²’æœ‰è¡¨æ ¼/æ²’æœ‰å¯é»åˆ—ï¼‰
      - "UNKNOWN"   : è¶…æ™‚ä»ç„¡æ³•åˆ¤æ–·
    """
    steps = int(max_wait_sec / 0.5)
    for _ in range(steps):
        try:
            # A) çœ‹æ˜¯å¦ç›´æ¥é¡¯ç¤ºã€ŒæŸ¥ç„¡è³‡æ–™ã€
            hdr = await iframe.query_selector(".page-header h3")
            if hdr:
                text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
                if "æŸ¥ç„¡è³‡æ–™" in text:
                    return "NO_DATA"

            # B) çœ‹æ˜¯å¦å·²æœ‰å¯é»çš„æ˜ç´°åˆ—
            table = await iframe.query_selector('table#jud.jub-table')
            if table:
                rows = await table.query_selector_all("tbody > tr")
                for r in rows:
                    link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
                    if link:
                        return "HAS_DATA"
        except:
            pass
        await asyncio.sleep(0.5)

    # è¶…æ™‚å¾Œä¿éšªåˆ¤æ–·
    try:
        table = await iframe.query_selector('table#jud.jub-table')
        if not table:
            return "NO_DATA"
        rows = await table.query_selector_all("tbody > tr")
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                return "HAS_DATA"
        return "NO_DATA"
    except:
        return "UNKNOWN"

async def get_total_result_count_hint(iframe):
    """
    ç”¨é€”ï¼šå¾çµæœé é¢æ–‡å­—ä¸­å˜—è©¦è§£æã€Œå…± X ç­†ã€çš„å®˜æ–¹ç¸½ç­†æ•¸æç¤ºã€‚
    å›å‚³ï¼šintï¼ˆç¸½ç­†æ•¸ï¼‰æˆ– Noneï¼ˆé é¢æœªé¡¯ç¤ºæˆ–ç„¡æ³•è§£æï¼‰ã€‚
    å‚™è¨»ï¼šç«™æ–¹åœ¨çµæœæ•¸ â‰¤ 20 ç­†æ™‚é€šå¸¸ä¸é¡¯ç¤ºç¸½æ•¸ï¼Œå› æ­¤å¸¸æœƒå›å‚³ Noneã€‚
    """
    html = await iframe.content()
    html = html.replace('\u00a0', ' ') # å°‡nbspæ›æˆä¸€èˆ¬ç©ºç™½
    m = re.search(r"å…±\s*([0-9,]+)\s*ç­†", html)
    if m:
        return int(m.group(1).replace(",", ""))
    return None

async def count_rows_on_current_page(iframe):
    """
    ç”¨é€”ï¼šè¨ˆç®—ã€Œç›®å‰é€™ä¸€é æ¸…å–®ã€çš„æœ‰æ•ˆç­†æ•¸ã€‚
    ä½œæ³•ï¼šæŠ“çµæœè¡¨æ ¼ â†’ é€åˆ—æª¢æŸ¥æ˜¯å¦å«æœ‰é€£åˆ°è©³æƒ…é çš„è¶…é€£çµï¼ˆid= / ty=JD / ty=JUDBOOKï¼‰ï¼Œæœ‰å°±ç®— 1 ç­†ã€‚
    å›å‚³ï¼šintï¼ˆè©²é çš„æœ‰æ•ˆåˆ—æ•¸ï¼‰ï¼›è‹¥æ‰¾ä¸åˆ°è¡¨æ ¼å‰‡å›å‚³ 0ã€‚
    """
    table = await get_result_table(iframe)
    if not table:
        return 0
    rows = await table.query_selector_all("tbody > tr")
    cnt = 0
    for r in rows:
        link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
        if link:
            cnt += 1
    return cnt

async def get_total_pages(iframe):
    """
    ç”¨é€”ï¼šç”±åˆ†é ä¸‹æ‹‰é¸å–®ï¼ˆ#ddlPageï¼‰ä¼°ç®—ç¸½é æ•¸ã€‚
    å›å‚³ï¼šintï¼ˆè‡³å°‘ç‚º 1ï¼‰ï¼›è‹¥æ²’æœ‰åˆ†é é¸å–®æˆ–æ²’æœ‰é¸é …å‰‡è¦–ç‚º 1ã€‚
    """
    select = await iframe.query_selector('select#ddlPage')
    if not select:
        return 1
    options = await select.query_selector_all('option')
    return len(options) if options else 1

async def select_page_by_index(iframe, page, target_index):
    """
    ç”¨é€”ï¼šåœ¨ã€ŒæŸ¥è©¢çµæœæ¸…å–®é ã€ä¸­ï¼Œé€éä¸‹æ‹‰åˆ†é é¸å–®(#ddlPage)åˆ‡æ›åˆ°æŒ‡å®šé ç¢¼ã€‚
    å‚™è¨»ï¼štarget_index å¾ 1 é–‹å§‹ï¼›æˆåŠŸè§¸ç™¼åˆ‡é å›å‚³ Trueï¼Œå¦å‰‡ Falseã€‚
    """
    select = await iframe.query_selector('select#ddlPage')
    if not select:
        return False

    options = await select.query_selector_all('option')
    if not options or target_index < 1 or target_index > len(options):
        return False

    opt_val = await options[target_index - 1].get_attribute('value')
    if not opt_val:
        return False
    await select.select_option(value=opt_val)
    try:
        await iframe.evaluate("""(sel)=>{ sel.dispatchEvent(new Event('change', {bubbles:true})); }""", select)
    except:
        pass
    await asyncio.sleep(random.uniform(*WAIT_PAGE_JUMP))
    await page.wait_for_timeout(1000)
    return True

async def goto_page(page, iframe, cur_page):
    try:
        return await select_page_by_index(iframe, page, cur_page)
    except Exception as e:
        print_debug(f"!!åˆ‡æ›åˆ†é å¤±æ•—: {e}")
        return False

# ====== æŸ¥ç„¡è³‡æ–™ï¼ˆQ003ï¼‰åµæ¸¬ & é€å‡ºæŸ¥è©¢ ======
async def is_no_data_q003(page):
    """
    åµæ¸¬ iframe src ç‚º ErrorPage.aspx?err=Q003ï¼Œæˆ–ä¸»é æ¨™é¡Œé¡¯ç¤ºã€æŸ¥ç„¡è³‡æ–™ã€ã€‚
    """
    iframe_elem = await page.query_selector("#iframe-data")
    if iframe_elem:
        src = (await iframe_elem.get_attribute("src")) or ""
        if "ErrorPage.aspx" in src and "err=Q003" in src:
            return True

    hdr = await page.query_selector(".page-header h3")
    if hdr:
        text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
        if "æŸ¥ç„¡è³‡æ–™" in text:
            return True
    return False

async def submit_and_get_iframe(page):
    """
    é€å‡ºæŸ¥è©¢ä¸¦å˜—è©¦å–å¾— iframe çµæœé ã€‚
    é‡ ErrorPage.aspx?err=Q003 æˆ–é æ¨™ã€æŸ¥ç„¡è³‡æ–™ã€â†’ (None, 'NO_DATA')
    æ­£å¸¸å–å¾—åˆ—è¡¨ iframe â†’ (iframe, 'OK')
    å…¶ä»–å¤±æ•— â†’ (None, 'UNKNOWN')
    """
    print_debug("é€å‡ºæŸ¥è©¢æ¢ä»¶...")
    await page.click("#btnQry")
    await asyncio.sleep(random.uniform(*WAIT_AFTER_CLICK_QUERY))

    for _ in range(60):  # æœ€å¤šç­‰ 30 ç§’
        # å…ˆåˆ¤æ–·æ˜¯å¦ã€æŸ¥ç„¡è³‡æ–™ã€
        if await is_no_data_q003(page):
            print_debug("ğŸ” æŸ¥ç„¡è³‡æ–™ï¼ˆErrorPage Q003 / é æ¨™ï¼‰ï¼ŒçµæŸæœ¬åƒæ•¸çµ„ã€‚")
            return None, "NO_DATA"

        # å˜—è©¦å–å¾—çµæœ iframe
        iframe_elem = await page.query_selector("#iframe-data")
        if iframe_elem:
            src = await iframe_elem.get_attribute("src")
            if src:
                print_debug(f"iframe src: {src}")
                if "qryresultlst.aspx" in src:
                    for f in page.frames:
                        if src in f.url or f.name == "iframe-data":
                            print_debug(f"å·²æ­£ç¢ºå–å¾— iframeï¼ŒURL: {f.url}")
                            return f, "OK"

        await asyncio.sleep(0.5)

    print_debug("æœªæ­£ç¢ºå–å¾— iframeï¼ˆé Q003ï¼‰ï¼Œæµç¨‹çµ‚æ­¢")
    return None, "UNKNOWN"

# ====== åƒæ•¸è¼‰å…¥èˆ‡ CSV å¯«å…¥ ======
def load_parameters_from_csv(filepath):
    df = pd.read_csv(filepath, dtype=str).fillna("")
    params_list = []
    for _, row in df.iterrows():
        params_list.append({
            "case_year": row["case_year"].strip(),
            "case_type": row["case_type"].strip(),
            "case_number_start": row["case_number_start"].strip(),
            "case_number_end": row["case_number_end"].strip(),
            "case_cause": row["case_cause"].strip(),
            "case_judgement": row["case_judgement"].strip(),
            "full_text": row["full_text"].strip(),
            "case_size_min": row["case_size_min"].strip(),
            "case_size_max": row["case_size_max"].strip(),
            "start_year": row["start_year"].strip(),
            "start_month": row["start_month"].zfill(2),
            "start_day": row["start_day"].zfill(2),
            "end_year": row["end_year"].strip(),
            "end_month": row["end_month"].zfill(2),
            "end_day": row["end_day"].zfill(2),
            "case_category": [x.strip() for x in row["case_category"].split(';') if x.strip()]
        })
    return params_list

def save_structured_result(data_dict):
    file_exists = os.path.exists(CSV_RESULT_PATH)
    with open(CSV_RESULT_PATH, 'a', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=list(data_dict.keys()))
        if not file_exists:
            writer.writeheader()
        writer.writerow(data_dict)

# ====== å››ç¨®è¼¸å‡º I/Oï¼ˆæ–°å¢ï¼‰ ======
def _ensure_csv_headers(path: str, headers: list):
    need_header = not os.path.exists(path)
    f = open(path, "a", newline="", encoding="utf-8-sig")
    w = csv.DictWriter(f, fieldnames=headers)
    if need_header:
        w.writeheader()
    # å›å‚³ writer èˆ‡æª”æ¡ˆ handleï¼ˆç”±å‘¼å«ç«¯é—œé–‰ï¼Œé¿å…é »ç¹é–‹é—œæª”ï¼‰
    return w, f

def append_jsonl(row: dict):
    with open(JSONL_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

def append_mgmt_csv(row: dict):
    headers = ["url","court","year","zi","number","case_type","judgment_type","cause","has_pdf","fetched_at"]
    w, fh = _ensure_csv_headers(MGMT_CSV_PATH, headers)
    try:
        w.writerow({k: row.get(k, "") for k in headers})
    finally:
        fh.close()

def append_db_csv(row: dict):
    headers = ["jud_key","url","court","year","zi","number","case_type","judgment_type","pdf_path","created_at"]
    w, fh = _ensure_csv_headers(DB_CSV_PATH, headers)
    try:
        w.writerow({k: row.get(k, "") for k in headers})
    finally:
        fh.close()

# ====== å¡«è¡¨ ======
async def fill_query_form(page, params):
    print_debug("è¼‰å…¥æŸ¥è©¢é é¢...")
    await page.goto(BASE_URL, timeout=60000)
    await asyncio.sleep(2)

    print_debug("å‹¾é¸æ¡ˆä»¶é¡åˆ¥...")
    for v in params["case_category"]:
        cb_selector = f'input[name="jud_sys"][value="{v}"]'
        await page.check(cb_selector)
        await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    print_debug("è¼¸å…¥è£åˆ¤æœŸé–“...")
    await page.fill("#dy1", params["start_year"])
    await page.fill("#dm1", params["start_month"])
    await page.fill("#dd1", params["start_day"])
    await page.fill("#dy2", params["end_year"])
    await page.fill("#dm2", params["end_month"])
    await page.fill("#dd2", params["end_day"])
    await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    print_debug("è¼¸å…¥è£åˆ¤å­—è™Ÿèˆ‡é—œéµå­—...")
    if params["case_year"]:
        await page.fill("#jud_year", params["case_year"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_type"]:
        await page.fill("#jud_case", params["case_type"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_number_start"]:
        await page.fill("#jud_no", params["case_number_start"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_cause"]:
        await page.fill("#jud_title", params["case_cause"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_judgement"]:
        await page.fill("#jud_jmain", params["case_judgement"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["full_text"]:
        await page.fill("#jud_kw", params["full_text"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_size_min"]:
        await page.fill("#KbStart", params["case_size_min"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_size_max"]:
        await page.fill("#KbEnd", params["case_size_max"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    await asyncio.sleep(WAIT_AFTER_FORM_INPUT)

# ====== å¤§å‡½å¼ï¼šæäº¤æŸ¥è©¢ä¸¦å–å¾—çµæœ iframeï¼ˆæ–°å¢ï¼‰ ======
async def submit_query_and_get_iframe(page, params):
    await fill_query_form(page, params)
    iframe, status = await submit_and_get_iframe(page)

    if status == "NO_DATA":
        # æ­£å¸¸æƒ…æ³ï¼šåƒæ•¸çµ„åˆçœŸçš„æ²’æœ‰è³‡æ–™ï¼Œä¸è¨˜ log
        return None, "NO_DATA"

    if iframe is None:
        # æŠ€è¡“æ€§å•é¡Œï¼šå¿…é ˆè¨˜ LOG-IFRAME
        log_stage("IFRAME", "æ­¤åƒæ•¸çµ„åˆç„¡æ³•å–å¾—çµæœ iframeï¼ˆéQ003ï¼‰", params=str(params)[:120])
        return None, "UNKNOWN"

    # ç­‰çµæœå°±ç·’
    state = await wait_results_state(iframe, max_wait_sec=20)
    if state == "NO_DATA":
        return None, "NO_DATA"
    if state == "UNKNOWN":
        # DOM è®Šå‹•/é€¾æ™‚ï¼Œç•¶æˆ 0 ç­†å›å ±ï¼ˆä¸ç•¶éŒ¯ï¼‰
        return None, "UNKNOWN"

    return iframe, "OK"

# ====== åˆ†é å¤§å‡½å¼ï¼ˆå–ä»£åŸ process_search_pagesï¼‰ ======
async def process_result_pages(page, iframe, context, params, params_pdf_dir=None):
    # â€”â€” ç¬¬ä¸€éšæ®µï¼šæˆåŠŸé€²å…¥åˆ†å±¤é ï¼ˆåˆ†å±¤é  = çµæœæ¸…å–®é ï¼‰â€”â€”
    log_stage("STAGE1", "å·²é€²å…¥çµæœæ¸…å–®é ï¼ˆåˆ†å±¤é ï¼‰")

    # å…ˆå˜—è©¦ã€Œå…± X ç­†ã€
    total_count_hint = await get_total_result_count_hint(iframe)
    final_total_count = None
    if total_count_hint is not None:
        final_total_count = total_count_hint
    else:
        # ä¼°ç®—ï¼šå–®é /æœ€å¾Œä¸€é å›æ¨
        try:
            total_pages = await get_total_pages(iframe)
            if total_pages <= 1:
                first_page_rows = await count_rows_on_current_page(iframe)
                final_total_count = first_page_rows
            else:
                # è·³æœ€å¾Œä¸€é æ•¸åˆ—
                cur_iframe = iframe
                await select_page_by_index(iframe, page, total_pages)
                await asyncio.sleep(1.0)
                cur_iframe = await get_iframe(page)
                last_rows = await count_rows_on_current_page(cur_iframe)

                # å›ç¬¬ä¸€é æ‹¿ page_size
                await select_page_by_index(cur_iframe, page, 1)
                await asyncio.sleep(1.0)
                cur_iframe = await get_iframe(page)
                page_size = await count_rows_on_current_page(cur_iframe) or 20
                final_total_count = (total_pages - 1) * page_size + last_rows

                # å›ç¬¬ä¸€é é–‹å§‹è™•ç†
                await select_page_by_index(cur_iframe, page, 1)
                await asyncio.sleep(0.8)
                iframe = await get_iframe(page)
        except Exception as e:
            log_stage("STAGE1", "ç„¡æ³•è¨ˆç®—ç¸½ç­†æ•¸ï¼ˆå°‡ä»å˜—è©¦æŠ“å–æœ¬é ï¼‰", error=str(e))

    processed_count = 0
    max_page = TEST_MODE_LIMIT_PAGE  # ä½ çš„æ¸¬è©¦ä¸Šé™

    for cur_page in range(1, max_page + 1):
        print_debug(f"==é–‹å§‹è™•ç†ç¬¬ {cur_page} é ==")

        # ç¿»é ï¼ˆç¬¬1é ä¸ç”¨ï¼‰
        if cur_page > 1:
            ok = await goto_page(page, iframe, cur_page)
            if not ok:
                break
            await asyncio.sleep(1.5)
            iframe = await get_iframe(page)
            if not iframe or "qryresultlst.aspx" not in (iframe.url or ""):
                print_debug("é é¢è·³è½‰å¾Œæ‰¾ä¸åˆ°æœ‰æ•ˆ iframeï¼Œåœæ­¢ã€‚")
                break
            # â€”â€” ç¬¬ä¸‰éšæ®µ LOGï¼šç¿»é æˆåŠŸ â€”â€”
            log_stage("STAGE3", "å‰å¾€ä¸‹ä¸€åˆ†é ", next_page=cur_page)

        # æ“·å–æœ¬é æ‰€æœ‰æ˜ç´° URLï¼ˆé€™è£¡ç›´æ¥æŠ“ elementï¼Œè®“æ˜ç´°éšæ®µå»é»ï¼‰
        table = await get_result_table(iframe)
        if not table:
            log_stage("URLS", "æŸ¥ç„¡çµæœ tableï¼ˆæœ¬é ï¼‰")
            break

        rows = await table.query_selector_all("tbody > tr")
        url_elems = []
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                url_elems.append(link)

        if not url_elems:
            log_stage("URLS", "æœªæ“·å–åˆ°ä»»ä½•æ˜ç´° URLï¼ˆè«‹æª¢æŸ¥ selector æˆ–é é¢è®Šæ›´ï¼‰")
            break
        else:
            log_stage("URLS", "å·²æ“·å–æ˜ç´° URL", count=len(url_elems))

        # â€”â€” æ˜ç´°éšæ®µï¼ˆç¬¬äºŒéšæ®µ LOG åœ¨è£¡é¢ï¼‰â€”â€”
        handled, hit_limit = await process_details(page, url_elems, context, params, params_pdf_dir)
        processed_count += handled

        # å–®é ä¸Šé™ï¼ˆä½ çš„ TEST_MODE_LIMIT_PER_PAGEï¼‰
        if hit_limit:
            log_stage("STAGE3", "é”åˆ°æœ¬åˆ†é è™•ç†ä¸Šé™", page_index=cur_page, handled=handled)

    return processed_count, (final_total_count if final_total_count is not None else processed_count)

# ====== æ˜ç´°å¤§å‡½å¼ï¼ˆæ–°å¢ï¼‰ ======
async def process_details(page, url_elements, context, params, params_pdf_dir=None):
    """
    å›å‚³ï¼š(æœ¬é è™•ç†æ•¸, æ˜¯å¦é”ä¸Šé™)
    - é€ç­†é–‹å•Ÿæ˜ç´° â†’ æ“·å–è³‡æ–™ â†’ ç”¢å‡ºæª”æ¡ˆï¼ˆJSONL / PDF or PNG / æ¥­ç®¡CSV / DB CSVï¼‰
    - ç”¢å‡ºæª”æ¡ˆå‰è¨˜éŒ„ LOG-STAGE2
    """
    base_dir = params_pdf_dir or PDF_SNAPSHOT_PATH
    os.makedirs(base_dir, exist_ok=True)

    handled = 0
    for idx, link in enumerate(url_elements, start=1):
        if idx > TEST_MODE_LIMIT_PER_PAGE:
            return handled, True

        try:
            # å˜—è©¦ç”¨æ–°é é–‹å•Ÿï¼Œå®¹å¿åŒé å°èˆª
            async with context.expect_page() as popup_info:
                try:
                    await link.evaluate("a => a.target = '_blank'")
                except:
                    pass
                await link.click(button="middle")
            try:
                popup = await popup_info.value
                page_like = popup
            except:
                popup = None
                page_like = page

            await page_like.bring_to_front()
            await page_like.wait_for_load_state("domcontentloaded", timeout=20000)
            await asyncio.sleep(random.uniform(*WAIT_DETAIL_CONTENT))
            await human_noise(page_like)

            # ====== åŸºæœ¬å‘½åèˆ‡åˆ†äº«é€£çµ ======
            share_link = clean_url(page_like.url)
            html = await page_like.content()
            soup = BeautifulSoup(html, "html.parser")

            # å–ã€Œè£åˆ¤å­—è™Ÿï¼è£åˆ¤æ—¥æœŸï¼è£åˆ¤æ¡ˆç”±ã€
            def get_value_by_label_bs(soup, label):
                for th, td in zip(soup.select("div.col-th"), soup.select("div.col-td")):
                    th_text = th.get_text(strip=True)
                    if label in th_text:
                        return td.get_text(strip=True)
                return ""

            title_text = get_value_by_label_bs(soup, "è£åˆ¤å­—è™Ÿ")
            date_text_raw = get_value_by_label_bs(soup, "è£åˆ¤æ—¥æœŸ")
            cause_text = get_value_by_label_bs(soup, "è£åˆ¤æ¡ˆç”±")

            # è¨˜éŒ„åˆ†äº«æ¸…å–®
            date_roc = date_text_raw or ""
            date_str = re.sub(r"æ°‘åœ‹\s*(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥",
                              lambda m: f"{int(m.group(1))}/{int(m.group(2)):02}/{int(m.group(3)):02}", date_roc)
            share_title = f"{date_str}_{(title_text or '').replace(' ', '')}"
            save_share_url(share_title, share_link or "")

            # è§£ææ°‘åœ‹æ—¥æœŸ â†’ yyy/mm/ddï¼ˆæª”åç”¨ï¼‰
            date_text = re.sub(
                r"æ°‘åœ‹\s*(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥",
                lambda m: f"{int(m.group(1))}/{int(m.group(2)):02}/{int(m.group(3)):02}",
                date_text_raw or ""
            )

            # ç°¡åŒ–æª”åï¼ˆç”¨è£åˆ¤å­—è™Ÿèˆ‡æ—¥æœŸï¼‰
            base_name = sanitize_filename((date_text or "date") + "_" + (title_text or "judgment"))
            filename_base = os.path.join(base_dir, base_name)
            pdf_path = filename_base + ".pdf"
            png_path = filename_base + ".png"

            # ====== ä¸‹è¼‰ PDFï¼ˆç«™å…§è½‰å­˜ï¼Œå¤±æ•—å‰‡æ‰“å° PDFï¼‰ï¼‹ PNG ======
            if not (os.path.exists(pdf_path) and os.path.exists(png_path)):
                try:
                    await download_via_site_pdf(page_like, pdf_path)
                    print_debug(f"å·²ç”¨ã€ç«™å…§è½‰å­˜PDFã€ä¸‹è¼‰ï¼š{pdf_path}")
                except Exception as e_download:
                    print_debug(f"ç«™å…§è½‰å­˜PDFå¤±æ•—ï¼Œæ”¹ç”¨æ‰“å°PDFå‚™æ´ï¼š{e_download}")
                    await save_pdf_snapshot(page_like, pdf_path)
                    print_debug(f"å·²å­˜PDF(å‚™æ´)ï¼š{pdf_path}")
                await save_png_snapshot(page_like, png_path)
                print_debug(f"å·²æˆªåœ–ï¼š{png_path}")

            # ====== è§’è‰²/æ¬„ä½è§£æï¼ˆæ²¿ç”¨ä½ åŸæœ¬é‚è¼¯ï¼‰ ======
            def extract_party_by_prefix(soup, prefix):
                prefix_clean = prefix.replace(' ', '').replace('\u00a0','')
                pattern = re.compile(rf"^{prefix_clean}")
                judgment_start_markers = ["ä¸Š", "ä¸Šåˆ—"]
                buffer = []
                collecting = False
                for div in soup.find_all("div"):
                    text_raw = div.get_text(" ", strip=True)
                    text = text_raw.replace('\u00a0', '').replace(' ', '')
                    if pattern.match(text):
                        collecting = True
                        cleaned = re.sub(pattern, '', text).strip()
                        if cleaned:
                            buffer.append(cleaned)
                    elif collecting:
                        if any(other in text for other in ["è²è«‹äºº","æ³•å®šä»£ç†äºº","ç›¸å°äºº","ä»£ç†äºº","ä¸Šè¨´äºº","æŠ—å‘Šäºº","é¸ä»»è¾¯è­·äºº","å†æŠ—å‘Šäºº"] if other != prefix):
                            break
                        if any(marker in text_raw for marker in judgment_start_markers):
                            break
                        if not text_raw.strip():
                            continue
                        cleaned = re.sub(rf"^{prefix}\\s*", '', text_raw.strip())
                        if cleaned != prefix:
                            buffer.append(cleaned)
                return ','.join(buffer).strip()

            applicant = extract_party_by_prefix(soup, "è²è«‹äºº")
            agent = extract_party_by_prefix(soup, "æ³•å®šä»£ç†äºº")
            respondent = extract_party_by_prefix(soup, "ç›¸å°äºº")

            # è£åˆ¤å­—è™Ÿç´°æ‹†ï¼ˆæ²¿ç”¨ä½ çš„ regexï¼‰
            pattern = (r'(?P<æ³•é™¢>.+?)\s*(?P<å¹´åº¦>\d{2,3})\s*å¹´åº¦(?P<å­—>\S+?)å­—ç¬¬\s*(?P<å­—è™Ÿ>\d+)\s*è™Ÿ(?P<åˆ‘æ°‘>(æ°‘äº‹|åˆ‘äº‹))?\s*(?P<è£å®šé¡å‹>è£å®š|åˆ¤æ±º|è£åˆ¤|å‘½ä»¤|è£åˆ¤|å…¶ä»–)?')
            match = re.match(pattern, title_text or "")
            court = match.group("æ³•é™¢") if match else ""
            year = match.group("å¹´åº¦") if match else ""
            case_word = match.group("å­—") if match else ""
            case_number = match.group("å­—è™Ÿ") if match else ""
            case_type = match.group("åˆ‘æ°‘") if match and match.group("åˆ‘æ°‘") else ""
            judgment_type = match.group("è£å®šé¡å‹") if match and match.group("è£å®šé¡å‹") else ""

            # å˜—è©¦æŠ“å–ã€Œæ­£æ–‡æ–‡å­—ã€ä¾› JSONLï¼ˆè‹¥æŠ“ä¸åˆ°å°±ä»¥æ‘˜è¦ç‚ºä¸»ï¼‰
            body_text = ""
            main_candidates = ["#jud-content",".jud","div.card-block","div#content"]
            for sel in main_candidates:
                node = soup.select_one(sel)
                if node:
                    body_text = node.get_text(" ", strip=True)
                    break
            if not body_text:
                body_text = soup.get_text(" ", strip=True)[:15000]  # é˜²å¤ªé•·

            # â€”â€” ç¬¬äºŒéšæ®µï¼šç”¢å‡ºæª”æ¡ˆå‰ LOG â€”â€”
            log_stage("STAGE2", "å³å°‡ç”¢å‡ºæª”æ¡ˆï¼ˆJSON/PDF/æ¥­ç®¡CSV/DBCSVï¼‰", url=share_link)

            # 1) JSONLï¼ˆå…¨æ–‡/æ‘˜è¦ï¼‰
            fetched_at = now_tpe_str()
            append_jsonl({
                "url": share_link,
                "fetched_at": fetched_at,
                "title": title_text,
                "date": date_text,
                "cause": cause_text,
                "applicant": applicant,
                "agent": agent,
                "respondent": respondent,
                "court": court,
                "year": year,
                "zi": case_word,
                "number": case_number,
                "case_type": case_type,
                "judgment_type": judgment_type,
                "pdf_path": pdf_path,
                "png_path": png_path,
                "body_text": body_text
            })

            # 2) æ¥­ç®¡ CSVï¼ˆç²¾ç°¡æ‘˜è¦ï¼‰
            append_mgmt_csv({
                "url": share_link,
                "court": court,
                "year": year,
                "zi": case_word,
                "number": case_number,
                "case_type": case_type,
                "judgment_type": judgment_type,
                "cause": cause_text,
                "has_pdf": os.path.exists(pdf_path),
                "fetched_at": fetched_at
            })

            # 3) è³‡æ–™åº« CSVï¼ˆæ‰å¹³ã€å¾ŒçºŒå¯æ­£è¦åŒ–ï¼‰
            jud_key = f"{court}-{year}-{case_word}-{case_number}".strip("-")
            append_db_csv({
                "jud_key": jud_key,
                "url": share_link,
                "court": court,
                "year": year,
                "zi": case_word,
                "number": case_number,
                "case_type": case_type,
                "judgment_type": judgment_type,
                "pdf_path": pdf_path,
                "created_at": fetched_at
            })

            # 4) ä½ åŸæœ¬çš„ã€Œçµæ§‹åŒ– CSVã€ï¼ˆä¿ç•™ï¼‰
            result = {
                "è£åˆ¤å­—è™Ÿ": title_text,
                "è£åˆ¤å­—è™Ÿ_æ³•é™¢": court,
                "è£åˆ¤å­—è™Ÿ_å¹´åº¦": year,
                "è£åˆ¤å­—è™Ÿ_å­—": case_word,
                "è£åˆ¤å­—è™Ÿ_å­—è™Ÿ": case_number,
                "è£åˆ¤å­—è™Ÿ_åˆ‘äº‹æ°‘äº‹": case_type,
                "è£åˆ¤å­—è™Ÿ_è£å®š": judgment_type,
                "è£åˆ¤æ—¥æœŸ": date_text,
                "è£åˆ¤æ¡ˆç”±": cause_text,
                "è²è«‹äºº": applicant,
                "æ³•å®šä»£ç†äºº": agent,
                "ç›¸å°äºº": respondent,
                "æ˜ç´°ç¶²å€": share_link,
            }
            save_structured_result(result)

            # é˜²çˆ¬æš«åœ
            if await is_busy_page(page_like):
                print_debug("!!! åµæ¸¬åˆ°é˜²çˆ¬èŸ²æµé‡é™åˆ¶ï¼Œæš«åœ 5 åˆ†é˜å†ç¹¼çºŒ...")
                if popup:
                    await page_like.close()
                await asyncio.sleep(WAIT_BUSY_PAGE)

            if popup:
                await page_like.close()

            handled += 1

        except Exception as e:
            # â€”â€” DETAIL å¤±æ•—å›å ± â€”â€”
            log_stage("DETAIL", "æ˜ç´°è™•ç†å¤±æ•—", error=str(e))
            try:
                if 'page_like' in locals() and page_like:
                    err_base = os.path.join(base_dir, f"error_{idx}")
                    await save_png_snapshot(page_like, err_base + ".png")
                    await save_pdf_snapshot(page_like, err_base + ".pdf")
                    try:
                        await page_like.close()
                    except:
                        pass
                await asyncio.sleep(WAIT_AFTER_ERROR)
            except:
                pass
            continue

    return handled, False

# ====== å–®çµ„åƒæ•¸æµç¨‹ï¼ˆæ•´åˆæ–°çš„å€å¡Šï¼‰ ======
async def run_scraper_with_params(params):
    async with async_playwright() as p:
        # ä¸‹è¼‰ä¸€å®šè¦é–‹ accept_downloads
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
        context = await browser.new_context(accept_downloads=True)
        page = await context.new_page()

        # â€”â€” ç€è¦½å™¨å•Ÿå‹•å›å ± â€”â€”
        log_stage("BOOT", "ç€è¦½å™¨å•Ÿå‹•æˆåŠŸ")

        try:
            # [ADD] ç‚ºé€™ä¸€çµ„åƒæ•¸å»ºç«‹å°ˆå±¬ PDF ç›®éŒ„
            params_pdf_dir = build_params_pdf_dir(params)

            # æŸ¥è©¢ â†’ å–å¾— iframeï¼ˆå« Q003ã€UNKNOWN è™•ç†ï¼‰
            iframe, status = await submit_query_and_get_iframe(page, params)

            # å¦‚æœé é¢ç›´æ¥å‘ˆç¾NO_DATAæˆ–æ˜¯ç„¡iframeï¼Œå‰‡ç›´æ¥å°‡åƒæ•¸çµ„åˆç´€éŒ„è‡³query_log.txt
            if status == "NO_DATA":
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return
            if status != "OK" or not iframe:
                # IFRAME æŠ€è¡“æ€§å¤±æ•—å·²åœ¨ submit_query_and_get_iframe() è¢« LOG-IFRAME è¨˜éŒ„
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return

            # åˆ†é  + æ˜ç´°ï¼ˆå«ä¸‰éšæ®µ LOGï¼‰
            processed_count, final_total_count = await process_result_pages(page, iframe, context, params, params_pdf_dir)

            # å¯«å…¥æŸ¥è©¢å½™ç¸½ç´€éŒ„
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), final_total_count)

        # éŒ¯èª¤ä¾‹å¤–
        except PlaywrightTimeoutError as e:
            print_debug(f"Timeoutç™¼ç”Ÿ: {e}")
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
        except Exception as e:
            print_debug("ä¸»æµç¨‹å‡ºç¾éŒ¯èª¤: " + traceback.format_exc())
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
        finally:
            print_debug("ç¨‹å¼çµæŸ")
            if browser:
                await browser.close()

# ====== æ‰¹æ¬¡æµç¨‹ ======
async def run_all_tasks():
    """
    å¾ CSV æª”æ¡ˆä¸­è®€å–æ‰€æœ‰æŸ¥è©¢åƒæ•¸çµ„ï¼Œä¾åºå‘¼å«çˆ¬èŸ²ä¸»æµç¨‹ã€‚
    """
    # ä¸€æ¬¡æ€§åˆå§‹åŒ–ï¼ˆä¸å†æ¯çµ„æ¸…ç©ºï¼‰
    ensure_headers()

    # åƒæ•¸æª”è·¯å¾‘æ›´ç©©å¥ï¼šå„ªå…ˆ params(name).csvï¼Œæ‰¾ä¸åˆ°ç”¨ params.csv
    preferred = os.path.join(SAVE_DIR, "params(name).csv")
    fallback = os.path.join(SAVE_DIR, "params.csv")
    if os.path.exists(preferred):
        csv_path = preferred
    elif os.path.exists(fallback):
        csv_path = fallback
    else:
        print("âš ï¸ éŒ¯èª¤ï¼šæœªæ‰¾åˆ° params(name).csv æˆ– params.csvï¼Œè«‹ä¸Šå‚³å¾Œé‡æ–°åŸ·è¡Œã€‚")
        from google.colab import files
        files.upload()
        if os.path.exists(preferred):
            csv_path = preferred
        elif os.path.exists(fallback):
            csv_path = fallback
        else:
            print("âŒ ä¸Šå‚³å¤±æ•—æˆ–ä»æœªæ‰¾åˆ°æª”æ¡ˆï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
            return

    params_list = load_parameters_from_csv(csv_path)
    for i, params in enumerate(params_list, 1):
        print_debug(f"========== é–‹å§‹ç¬¬ {i} çµ„åƒæ•¸ ==========")
        await run_scraper_with_params(params)

# ====== å…¥å£ ======
if __name__ == "__main__":
    await run_all_tasks()


''' ä¸‹è¼‰pdfè³‡æ–™å¤¾
# 1) æ‰“åŒ…
!zip -r /content/crawler_outputs.zip \
  /content/PDF_Snapshots \
  /content/judgments.jsonl \
  /content/mgmt_requirements.csv \
  /content/db_records.csv \
  /content/judgments.csv \
  /content/Page_URL_List.txt \
  /content/Query_Log.txt 2>/dev/null

# 2) æ›è¼‰ Drive ä¸¦æ¬éå»
from google.colab import drive
drive.mount('/content/drive')

!mkdir -p "/content/drive/MyDrive/å¸æ³•çˆ¬èŸ²è¼¸å‡º"
!cp -n /content/crawler_outputs.zip "/content/drive/MyDrive/å¸æ³•çˆ¬èŸ²è¼¸å‡º/"
print("å·²æ¬åˆ°ï¼š/content/drive/MyDrive/å¸æ³•çˆ¬èŸ²è¼¸å‡º/crawler_outputs.zip")
'''
