# -*- coding: utf-8 -*-
"""
fj_judgment_crawler_colab.py
å¸æ³•é™¢åˆ¤æ±ºæ›¸æŸ¥è©¢è‡ªå‹•çˆ¬èŸ²ï¼ˆColab ç‰ˆ, æ”¯æ´åˆ†é èˆ‡PDF/PNGå¿«ç…§ï¼Œä¸¦ç”¢ç”Ÿæ˜ç´°é åˆ†äº«URLåŒ¯ç¸½ï¼‰
- é©ç”¨ Google Colabï¼Œæ‰€æœ‰è¼¸å‡ºå„²å­˜è‡³ /content
- è‡ªå‹•å®‰è£ Playwright èˆ‡ Chromium
"""

# ====== Colab ç’°å¢ƒåˆå§‹åŒ–ï¼ˆç©©å®šç‰ˆï¼‰======
import sys, os, asyncio, pandas as pd, time, traceback, re, random, csv, json
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from datetime import datetime

try:
    from zoneinfo import ZoneInfo  # Python 3.9+
except:
    ZoneInfo = None

# ---- å®‰è£ä¸­æ–‡å­—å‹ï¼ˆé˜²æ­¢ä¸­æ–‡æ–¹æ ¼ï¼‰----
!apt-get install -y fonts-noto-cjk > /dev/null

# ---- å®‰è£å¿…è¦å¥—ä»¶ï¼ˆåŒ…å« Playwright èˆ‡ç€è¦½å™¨ä¾è³´ï¼‰----
!pip install -U playwright beautifulsoup4 > /dev/null
!playwright install --with-deps chromium

# ---- å»ºç«‹ Colab å°ˆç”¨å„²å­˜è·¯å¾‘ ----
SAVE_DIR = "/content"
os.makedirs(SAVE_DIR, exist_ok=True)

# ---- åŒ¯å…¥ Playwright æ¨¡çµ„ ----
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

# ====== å¸¸æ•¸ ======
BASE_URL = "https://judgment.judicial.gov.tw/FJUD/Default_AD.aspx"

WAIT_BEFORE_CHECKBOX = (0.5, 1.2)
WAIT_AFTER_FORM_INPUT = 1
WAIT_AFTER_CLICK_QUERY = (1, 2)
WAIT_HUMAN_NOISE = (0.5, 1.5)
WAIT_DETAIL_CONTENT = (10, 15)
WAIT_PAGE_JUMP = (3, 6)
WAIT_BUSY_PAGE = 300
WAIT_AFTER_ERROR = 3

# é¸æ“‡éœ€è¦çš„æª”æ¡ˆé¡å‹
OUTPUT_OPTIONS = { "FULL_JSONL": True ,
           "SPLIT_JSONL": True ,
           "PDF": True ,
           "DB_CSV": True ,
           "MGMT_CSV": True ,
           "STRUCTURED_CSV": True }

PARTY_ROLE_MARKERS = ["è²è«‹äºº","æ³•å®šä»£ç†äºº","ç›¸å°äºº","ä»£ç†äºº","ä¸Šè¨´äºº","æŠ—å‘Šäºº",
            "é¸ä»»è¾¯è­·äºº","å†æŠ—å‘Šäºº","è¨´è¨Ÿä»£ç†äºº","åŸå‘Š","è¢«å‘Š","å‚µå‹™äºº",
            "å‚µæ¬Šäºº","åƒåŠ äºº"]

judgment_start_markers = ["ä¸Š","ä¸Šåˆ—","ä¸»æ–‡", "ç†ç”±", "è£å®šå¦‚ä¸‹", "åˆ¤æ±ºå¦‚ä¸‹","ä¸€ã€","äºŒã€",
              "ä»¥ä¸Šæ­£æœ¬", "èªªæ˜", "æ­·å¯©è£åˆ¤", "æ¡ˆä»¶ç›®å‰", "ç›¸é—œæ³•æ¢"
]

# L1ï¼ˆä¸»æ–‡ã€äº‹å¯¦ã€ç†ç”±ï¼‰æ”¯æ´å…§å«ç©ºç™½
L1_PATTERN = r'(ä¸»\s*æ–‡|äº‹\s*å¯¦|ç†\s*ç”±)'
# L2ï¼ˆå£¹è²³åƒâ€¦ï¼‰
L2_PATTERN = r'(å£¹|è²³|åƒ|è‚†|ä¼|é™¸|æŸ’|æŒ|ç–|æ‹¾)ã€'
# L3ï¼ˆä¸€äºŒä¸‰â€¦ï¼‰
L3_PATTERN = r'(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å)ã€'

HEADING_PATTERN = re.compile(
    rf'^\s*({L1_PATTERN}|{L2_PATTERN}|{L3_PATTERN})\s*$',
    re.MULTILINE
)

# è½æ¬¾ï¼ˆä¸­é–“å¯èƒ½æœ‰å…¨å½¢æˆ–åŠå½¢ç©ºç™½ï¼‰
ENDING_PATTERN = re.compile(
    r'ä¸­\s*è¯\s*æ°‘\s*åœ‹|æ›¸è¨˜å®˜|æ³•å®˜|å¸æ³•äº‹å‹™å®˜'
)


# æ¸¬è©¦æ¨¡å¼ä¸Šé™ï¼ˆæ­£å¼è·‘è«‹è¨­å¤§æˆ–ç§»é™¤ï¼‰
TEST_MODE_LIMIT_PAGE = 3
TEST_MODE_LIMIT_PER_PAGE = 2

BUSY_MSGS = ["ç³»çµ±å¿™ç¢Œä¸­","æ“ä½œéæ–¼é »ç¹", "è«‹æŒ‰ã€Œé‡æ–°æ•´ç†ã€", "è«‹ç¨å€™å†è©¦", "ç³»çµ±å¿™ç¢Œä¸­ï¼Œè«‹æŒ‰ã€Œé‡æ–°æ•´ç†ã€ã€‚"]

PAGE_URL_LIST_PATH = os.path.join(SAVE_DIR, "Page_URL_List.txt")
CSV_RESULT_PATH = os.path.join(SAVE_DIR, "judgments.csv")
QUERY_LOG_PATH = os.path.join(SAVE_DIR, "Query_Log.txt")  # å„åƒæ•¸çµ„çš„å½™ç¸½ç´€éŒ„
PDF_SNAPSHOT_PATH = os.path.join(SAVE_DIR, "PDF_Snapshots")
os.makedirs(PDF_SNAPSHOT_PATH, exist_ok=True)

# ====== å››ç¨®è¼¸å‡ºè·¯å¾‘ï¼ˆæ–°å¢ï¼‰ ======
JSONL_FULL_PATH = os.path.join(SAVE_DIR, "judgments_full.jsonl")
JSONL_SPLIT_PATH = os.path.join(SAVE_DIR, "judgments_split.jsonl")                 # å…¨æ–‡ JSONLï¼ˆé€ç­†è¿½åŠ ï¼‰
MGMT_CSV_PATH = os.path.join(SAVE_DIR, "mgmt_requirements.csv")            # æ¥­ç®¡éœ€æ±‚æª”ï¼ˆçµæ§‹åŒ–æ‘˜è¦ï¼‰
DB_CSV_PATH   = os.path.join(SAVE_DIR, "db_records.csv")               # è³‡æ–™åº«æª”ï¼ˆæ­£è¦åŒ–å‰çš„æ‰å¹³è¡¨ï¼‰

# ====== çµ±ä¸€æµç¨‹åŒ– LOGï¼ˆæ–°å¢ï¼‰ ======
def log_stage(tag: str, msg: str, **kv):
    tail = " ".join([f"{k}={v}" for k, v in kv.items()]) if kv else ""
    print(f"LOG-{tag} | {msg}" + (f" | {tail}" if tail else ""))

# ====== å°å·¥å…· ======
def print_debug(msg):
    print("[DEBUG]", msg)

# å°‡æ–‡å­—è½‰æˆå®‰å…¨æª”å
def slugify(value: str) -> str:
    banned = '\\/:*?"<>|'
    for ch in banned:
        value = value.replace(ch, "_")
    value = "_".join(value.split())           # ç©ºç™½â†’åº•ç·š
    value = re.sub(r"_+", "_", value).strip("_")
    return value[:80]                          # é¿å…éé•·

# è‹¥æª”åå·²å­˜åœ¨ï¼Œå°±è‡ªå‹•åŠ  _2, _3... é¿å…è¦†è“‹
def uniquify(path: str) -> str:
    base, ext = os.path.splitext(path)
    i, new = 2, path
    while os.path.exists(new):
        new = f"{base}_{i}{ext}"
        i += 1
    return new

# å»ºç«‹ã€Œæœ¬æ¬¡åƒæ•¸å°ˆå±¬çš„ PDF ç›®éŒ„ã€
def build_params_pdf_dir(params: dict) -> str:
    # æŠŠå¸¸ç”¨æ¢ä»¶çµ„æˆå¯è®€ç‰‡æ®µï¼ˆå¤ªé•·æœƒè¢« slugify æˆªæ–·ï¼‰
    date_range = f"{params.get('start_year','')}{params.get('start_month','')}{params.get('start_day','')}" \
                 f"-{params.get('end_year','')}{params.get('end_month','')}{params.get('end_day','')}"
    parts = [
        f"date-{date_range}",
        f"year-{params.get('case_year','')}" if params.get('case_year') else "",
        f"type-{params.get('case_type','')}" if params.get('case_type') else "",
        f"no-{params.get('case_number_start','')}_{params.get('case_number_end','')}" if (params.get('case_number_start') or params.get('case_number_end')) else "",
        f"cause-{params.get('case_cause','')}" if params.get('case_cause') else "",
        f"kw-{params.get('full_text','')}" if params.get('full_text') else "",
    ]
    human = slugify("__".join([p for p in parts if p]) or "params")
    # ç”¨å°åŒ—æ™‚é–“åŠ å€‹æ™‚é–“æˆ³ï¼Œè®“æ¯æ¬¡åŸ·è¡Œä¸äº’ç›¸è¦†è“‹ï¼ˆä¹Ÿæ›´å¥½è¿½è¹¤ï¼‰
    ts = datetime.now(ZoneInfo("Asia/Taipei")).strftime("%Y%m%d_%H%M%S") if ZoneInfo else datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    folder = os.path.join(PDF_SNAPSHOT_PATH, f"{ts}__{human}")
    os.makedirs(folder, exist_ok=True)
    return folder

def now_tpe_str():
    if ZoneInfo:
        t = datetime.now(ZoneInfo("Asia/Taipei"))
    else:
        t = datetime.utcnow()
    return t.strftime("%Y-%m-%d %H:%M:%S")

def ensure_headers():
    # åªåœ¨æª”æ¡ˆä¸å­˜åœ¨æ™‚è£œä¸Šè¡¨é ­
    if not os.path.exists(PAGE_URL_LIST_PATH):
        with open(PAGE_URL_LIST_PATH, 'w', encoding='utf-8') as f:
            f.write('æ¨™é¡Œ\tåˆ†äº«ç¶²å€\n')
    if not os.path.exists(QUERY_LOG_PATH):
        with open(QUERY_LOG_PATH, 'w', encoding='utf-8') as f:
            f.write('å…¨æ–‡å…§å®¹\tè£åˆ¤æ¡ˆç”±\tæŸ¥è©¢æ™‚é–“(å°åŒ—)\tç¸½ç­†æ•¸\n')

def append_query_log(full_text, case_cause, tpe_time_str, total_count):
    with open(QUERY_LOG_PATH, 'a', encoding='utf-8') as f:
        f.write(f"{full_text}\t{case_cause}\t{tpe_time_str}\t{total_count}\n")

def sanitize_filename(name):
    return re.sub(r'[\\/:*?"<>|]+', '_', name)

def clean_url(url):
    # ç§»é™¤ç¶²å€è£¡çš„ &ot=in æˆ– ot=in
    if not url:
        return ""
    parts = urlparse(url)
    qs = parse_qs(parts.query)
    qs.pop('ot', None)
    qs_str = urlencode(qs, doseq=True)
    new_parts = parts._replace(query=qs_str)
    clean = urlunparse(new_parts)
    if clean.endswith('?'):
        clean = clean[:-1]
    return clean

def save_share_url(title, url):
    # å–®ç­†å³æ™‚å¯«å…¥ï¼ˆä¸å†æ¯çµ„æ¸…ç©ºï¼‰
    with open(PAGE_URL_LIST_PATH, 'a', encoding='utf-8') as f:
        f.write(f'{title}\t{url}\n')

def normalize_title(raw_title):
    t = raw_title.strip()
    t = re.sub(r'\s+', '', t)
    return t  # ä¿ç•™ï¼šä¸»æ–‡ / äº‹å¯¦ / å£¹ã€ / ä¸€ã€


def extract_sections_dict(full_text: str):
    """
    è§£æçµæ§‹ â†’ ç›´æ¥åšæˆ dictï¼š{ "ä¸»æ–‡": "...", "äº‹å¯¦": "...", "ä¸€ã€": "...", ... }
    """
    matches = list(HEADING_PATTERN.finditer(full_text))
    result = {}

    if len(matches) == 0:
        # æ²’ä»»ä½•çµæ§‹ â†’ æ•´æ®µçµ¦ default key
        end = ENDING_PATTERN.search(full_text)
        cleaned = full_text if not end else full_text[:end.start()]
        result["å…¨æ–‡"] = cleaned.strip()
        return result

    for idx, m in enumerate(matches):
        raw_title = m.group(0)
        title = normalize_title(raw_title)

        start = m.end()
        end = matches[idx+1].start() if idx+1 < len(matches) else len(full_text)
        block = full_text[start:end]

        # æœ€æœ«æ®µ â†’ ç”¨è½æ¬¾åˆ‡æ‰
        if idx == len(matches) - 1:
            end2 = ENDING_PATTERN.search(block)
            if end2:
                block = block[:end2.start()]

        block = block.strip()

        # âš  æœ€é‡è¦ï¼šå…§å®¹ä¸å†å«æ¨™é¡Œ â†’ å› ç‚º regex å·²åˆ‡æ‰
        result[title] = block

    return result


def build_structured_json(full_text: str):
    """
    æœ€çµ‚ç‰ˆæœ¬ï¼šè¼¸å‡ºçœŸæ­£çš„çµæ§‹åŒ–æ¬„ä½ JSON æ ¼å¼
    """
    sections = extract_sections_dict(full_text)
    return {
        "full_text": full_text,
        "sections": sections   # <-- dictï¼Œè€Œä¸æ˜¯ list
    }
# ====== é˜²çˆ¬/äººé¡åŒ–æ“ä½œ ======
async def is_busy_page(page):
    html = await page.content()
    return any(msg in html for msg in BUSY_MSGS)

async def human_noise(page):
    x = random.randint(50, 600)
    y = random.randint(200, 700)
    await page.mouse.move(x, y)
    if random.random() > 0.5:
        await page.mouse.wheel(0, random.randint(200, 1200))
    if random.random() > 0.7:
        await page.mouse.click(random.randint(100, 800), random.randint(200, 800))
    await asyncio.sleep(random.uniform(*WAIT_HUMAN_NOISE))

# ====== å¿«ç…§ ======
async def save_pdf_snapshot(page, filename):
    await page.pdf(path=filename, format="A4", print_background=True)

async def save_png_snapshot(page, filename):
    await page.screenshot(path=filename, full_page=True)

# ====== ç«™å…§è½‰å­˜PDFï¼ˆå„ªå…ˆï¼‰ ======
async def download_via_site_pdf(popup, filename):
    """
    å„ªå…ˆå˜—è©¦é»æ“Šã€Œè½‰å­˜PDFã€æˆ–ç›¸è¿‘æ–‡æ¡ˆçš„æŒ‰éˆ•ï¼›æˆåŠŸå‰‡ä»¥ expect_download ä¸‹è¼‰ä¿å­˜ã€‚
    æ‰¾ä¸åˆ°æˆ–å¤±æ•—å°±ä¸Ÿå‡ºä¾‹å¤–ï¼Œå‘¼å«ç«¯å† fallback ç”¨ page.pdfã€‚
    """
    # å¯èƒ½éœ€è¦å…ˆå±•é–‹é¸å–®
    for menu_sel in ["#moreActions", "button:has-text('æ›´å¤š')", "text=æ›´å¤šåŠŸèƒ½"]:
        menu = popup.locator(menu_sel)
        if await menu.count() > 0:
            try:
                await menu.first.click()
                await asyncio.sleep(0.3)
            except:
                pass

    candidates = [
        "text=è½‰å­˜PDF", "text=åŒ¯å‡ºPDF", "text=å¦å­˜ç‚ºPDF",
        "a:has-text('PDF')", "button:has-text('PDF')",
        "[aria-label*='PDF']",
        "#hlExportPDF", "a[href*='.pdf']", "a[href*='toPDF']"
    ]
    target = None
    for sel in candidates:
        loc = popup.locator(sel)
        if await loc.count() > 0:
            target = loc.first
            break
    if not target:
        raise RuntimeError("æœªæ‰¾åˆ°ç«™å…§ã€è½‰å­˜/åŒ¯å‡ºPDFã€æŒ‰éˆ•")

    async with popup.expect_download() as dl_info:
        await target.click()
    download = await dl_info.value
    await download.save_as(filename)
    return filename

# ====== iframe èˆ‡åˆ—è¡¨ ======
async def get_iframe(page):
    """
    ç”¨é€”ï¼šåœ¨å¤–å±¤ Page ä¸­å°‹æ‰¾ä¸¦å›å‚³ã€ŒæŸ¥è©¢çµæœæ¸…å–®ã€æ‰€ä½¿ç”¨çš„ iframe ç‰©ä»¶ã€‚
    åˆ¤æ–·æ–¹å¼ï¼šæŠ“åˆ° <iframe id="iframe-data">ï¼Œå…¶ src éœ€åŒ…å« 'qryresultlst.aspx'ï¼ˆæ¸…å–®é ï¼‰ã€‚
    å›å‚³ï¼šPlaywright Frameï¼›è‹¥åœ¨ç´„ 30 ç§’å…§æ‰¾ä¸åˆ°å‰‡å›å‚³ Noneã€‚
    """
    for _ in range(60):
        iframe_elem = await page.query_selector("#iframe-data")
        if iframe_elem:
            src = await iframe_elem.get_attribute("src")
            print_debug(f"iframe src: {src}")
            if src and "qryresultlst.aspx" in src:
                for f in page.frames:
                    if src in f.url or f.name == "iframe-data":
                        print_debug(f"å·²æ­£ç¢ºå–å¾— iframeï¼ŒURL: {f.url}")
                        return f
        await asyncio.sleep(0.5)
    return None

async def get_result_table(iframe):
    """
    ç”¨é€”ï¼šåœ¨çµæœ iframe ä¸­å°‹æ‰¾æ¸…å–®è¡¨æ ¼ <table id="jud" class="jub-table"> ä¸¦å›å‚³å…ƒç´ æ§åˆ¶æŸ„ã€‚
    å›å‚³ï¼šElementHandle æˆ– Noneï¼ˆåœ¨ç­‰å¾…æœŸé™å…§æ‰¾ä¸åˆ°æ™‚ï¼‰ã€‚
    èªªæ˜ï¼šä»¥ 0.5 ç§’è¼ªè©¢ä¸€æ¬¡ã€æœ€å¤šç´„ 30 ç§’ï¼Œä¸¦å®¹å¿ iframe åœ¨è¼‰å…¥éç¨‹ä¸­çŸ­æš«å¤±æ•ˆã€‚
    """
    for _ in range(60):  # æœ€å¤šç­‰ 30 ç§’
        try:
            table = await iframe.query_selector('table#jud.jub-table')
            if table:
                return table
        except:
            pass
        await asyncio.sleep(0.5)
    return None

async def iframe_has_no_data_by_header_or_table(iframe):
    """
    å‚³çµ±å¿«é€Ÿåˆ¤æ–·ï¼šçœ‹ <h3>æŸ¥ç„¡è³‡æ–™</h3> æˆ–æ²’æœ‰æ˜ç´°é€£çµã€‚
    ç›®å‰ä¸»æµç¨‹æ”¹ç”¨ wait_results_stateï¼Œé€™å€‹å‡½å¼ä¿ç•™å‚™ç”¨ã€‚
    """
    try:
        hdr = await iframe.query_selector(".page-header h3")
        if hdr:
            text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
            if "æŸ¥ç„¡è³‡æ–™" in text:
                return True
        table = await get_result_table(iframe)
        if not table:
            return True
        rows = await table.query_selector_all("tbody > tr")
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                return False
        return True
    except:
        return True

async def wait_results_state(iframe, max_wait_sec=20):
    """
    ç­‰å¾…åˆ—è¡¨çµæœã€Œå°±ç·’ã€ï¼Œå›å‚³ï¼š
      - "HAS_DATA"  : æ‰¾åˆ°å¯é»æ“Šçš„æ˜ç´°åˆ—
      - "NO_DATA"   : æ˜ç¢ºé¡¯ç¤ºæŸ¥ç„¡è³‡æ–™ï¼ˆæˆ–æ²’æœ‰è¡¨æ ¼/æ²’æœ‰å¯é»åˆ—ï¼‰
      - "UNKNOWN"   : è¶…æ™‚ä»ç„¡æ³•åˆ¤æ–·
    """
    steps = int(max_wait_sec / 0.5)
    for _ in range(steps):
        try:
            # A) çœ‹æ˜¯å¦ç›´æ¥é¡¯ç¤ºã€ŒæŸ¥ç„¡è³‡æ–™ã€
            hdr = await iframe.query_selector(".page-header h3")
            if hdr:
                text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
                if "æŸ¥ç„¡è³‡æ–™" in text:
                    return "NO_DATA"

            # B) çœ‹æ˜¯å¦å·²æœ‰å¯é»çš„æ˜ç´°åˆ—
            table = await iframe.query_selector('table#jud.jub-table')
            if table:
                rows = await table.query_selector_all("tbody > tr")
                for r in rows:
                    link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
                    if link:
                        return "HAS_DATA"
        except:
            pass
        await asyncio.sleep(0.5)

    # è¶…æ™‚å¾Œä¿éšªåˆ¤æ–·
    try:
        table = await iframe.query_selector('table#jud.jub-table')
        if not table:
            return "NO_DATA"
        rows = await table.query_selector_all("tbody > tr")
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                return "HAS_DATA"
        return "NO_DATA"
    except:
        return "UNKNOWN"

async def get_total_result_count_hint(iframe):
    """
    ç”¨é€”ï¼šå¾çµæœé é¢æ–‡å­—ä¸­å˜—è©¦è§£æã€Œå…± X ç­†ã€çš„å®˜æ–¹ç¸½ç­†æ•¸æç¤ºã€‚
    å›å‚³ï¼šintï¼ˆç¸½ç­†æ•¸ï¼‰æˆ– Noneï¼ˆé é¢æœªé¡¯ç¤ºæˆ–ç„¡æ³•è§£æï¼‰ã€‚
    å‚™è¨»ï¼šç«™æ–¹åœ¨çµæœæ•¸ â‰¤ 20 ç­†æ™‚é€šå¸¸ä¸é¡¯ç¤ºç¸½æ•¸ï¼Œå› æ­¤å¸¸æœƒå›å‚³ Noneã€‚
    """
    html = await iframe.content()
    html = html.replace('\u00a0', ' ') # å°‡nbspæ›æˆä¸€èˆ¬ç©ºç™½
    m = re.search(r"å…±\s*([0-9,]+)\s*ç­†", html)
    if m:
        return int(m.group(1).replace(",", ""))
    return None

async def count_rows_on_current_page(iframe):
    """
    ç”¨é€”ï¼šè¨ˆç®—ã€Œç›®å‰é€™ä¸€é æ¸…å–®ã€çš„æœ‰æ•ˆç­†æ•¸ã€‚
    ä½œæ³•ï¼šæŠ“çµæœè¡¨æ ¼ â†’ é€åˆ—æª¢æŸ¥æ˜¯å¦å«æœ‰é€£åˆ°è©³æƒ…é çš„è¶…é€£çµï¼ˆid= / ty=JD / ty=JUDBOOKï¼‰ï¼Œæœ‰å°±ç®— 1 ç­†ã€‚
    å›å‚³ï¼šintï¼ˆè©²é çš„æœ‰æ•ˆåˆ—æ•¸ï¼‰ï¼›è‹¥æ‰¾ä¸åˆ°è¡¨æ ¼å‰‡å›å‚³ 0ã€‚
    """
    table = await get_result_table(iframe)
    if not table:
        return 0
    rows = await table.query_selector_all("tbody > tr")
    cnt = 0
    for r in rows:
        link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
        if link:
            cnt += 1
    return cnt

async def get_total_pages(iframe):
    """
    ç”¨é€”ï¼šç”±åˆ†é ä¸‹æ‹‰é¸å–®ï¼ˆ#ddlPageï¼‰ä¼°ç®—ç¸½é æ•¸ã€‚
    å›å‚³ï¼šintï¼ˆè‡³å°‘ç‚º 1ï¼‰ï¼›è‹¥æ²’æœ‰åˆ†é é¸å–®æˆ–æ²’æœ‰é¸é …å‰‡è¦–ç‚º 1ã€‚
    """
    select = await iframe.query_selector('select#ddlPage')
    if not select:
        return 1
    options = await select.query_selector_all('option')
    return len(options) if options else 1

async def select_page_by_index(iframe, page, target_index):
    """
    ç”¨é€”ï¼šåœ¨ã€ŒæŸ¥è©¢çµæœæ¸…å–®é ã€ä¸­ï¼Œé€éä¸‹æ‹‰åˆ†é é¸å–®(#ddlPage)åˆ‡æ›åˆ°æŒ‡å®šé ç¢¼ã€‚
    å‚™è¨»ï¼štarget_index å¾ 1 é–‹å§‹ï¼›æˆåŠŸè§¸ç™¼åˆ‡é å›å‚³ Trueï¼Œå¦å‰‡ Falseã€‚
    """
    select = await iframe.query_selector('select#ddlPage')
    if not select:
        return False

    options = await select.query_selector_all('option')
    if not options or target_index < 1 or target_index > len(options):
        return False

    opt_val = await options[target_index - 1].get_attribute('value')
    if not opt_val:
        return False
    await select.select_option(value=opt_val)
    try:
        await iframe.evaluate("""(sel)=>{ sel.dispatchEvent(new Event('change', {bubbles:true})); }""", select)
    except:
        pass
    await asyncio.sleep(random.uniform(*WAIT_PAGE_JUMP))
    await page.wait_for_timeout(1000)
    return True

async def goto_page(page, iframe, cur_page):
    try:
        return await select_page_by_index(iframe, page, cur_page)
    except Exception as e:
        print_debug(f"!!åˆ‡æ›åˆ†é å¤±æ•—: {e}")
        return False

# ====== æŸ¥ç„¡è³‡æ–™ï¼ˆQ003ï¼‰åµæ¸¬ & é€å‡ºæŸ¥è©¢ ======
async def is_no_data_q003(page):
    """
    åµæ¸¬ iframe src ç‚º ErrorPage.aspx?err=Q003ï¼Œæˆ–ä¸»é æ¨™é¡Œé¡¯ç¤ºã€æŸ¥ç„¡è³‡æ–™ã€ã€‚
    """
    iframe_elem = await page.query_selector("#iframe-data")
    if iframe_elem:
        src = (await iframe_elem.get_attribute("src")) or ""
        if "ErrorPage.aspx" in src and "err=Q003" in src:
            return True

    hdr = await page.query_selector(".page-header h3")
    if hdr:
        text = (await hdr.inner_text() or "").replace("\u00a0", " ").strip()
        if "æŸ¥ç„¡è³‡æ–™" in text:
            return True
    return False

async def submit_and_get_iframe(page):
    """
    é€å‡ºæŸ¥è©¢ä¸¦å˜—è©¦å–å¾— iframe çµæœé ã€‚
    é‡ ErrorPage.aspx?err=Q003 æˆ–é æ¨™ã€æŸ¥ç„¡è³‡æ–™ã€â†’ (None, 'NO_DATA')
    æ­£å¸¸å–å¾—åˆ—è¡¨ iframe â†’ (iframe, 'OK')
    å…¶ä»–å¤±æ•— â†’ (None, 'UNKNOWN')
    """
    print_debug("é€å‡ºæŸ¥è©¢æ¢ä»¶...")
    await page.click("#btnQry")
    await asyncio.sleep(random.uniform(*WAIT_AFTER_CLICK_QUERY))

    for _ in range(60):  # æœ€å¤šç­‰ 30 ç§’
        # å…ˆåˆ¤æ–·æ˜¯å¦ã€æŸ¥ç„¡è³‡æ–™ã€
        if await is_no_data_q003(page):
            print_debug("ğŸ” æŸ¥ç„¡è³‡æ–™ï¼ˆErrorPage Q003 / é æ¨™ï¼‰ï¼ŒçµæŸæœ¬åƒæ•¸çµ„ã€‚")
            return None, "NO_DATA"

        # å˜—è©¦å–å¾—çµæœ iframe
        iframe_elem = await page.query_selector("#iframe-data")
        if iframe_elem:
            src = await iframe_elem.get_attribute("src")
            if src:
                print_debug(f"iframe src: {src}")
                if "qryresultlst.aspx" in src:
                    for f in page.frames:
                        if src in f.url or f.name == "iframe-data":
                            print_debug(f"å·²æ­£ç¢ºå–å¾— iframeï¼ŒURL: {f.url}")
                            return f, "OK"

        await asyncio.sleep(0.5)

    print_debug("æœªæ­£ç¢ºå–å¾— iframeï¼ˆé Q003ï¼‰ï¼Œæµç¨‹çµ‚æ­¢")
    return None, "UNKNOWN"

# ====== åƒæ•¸è¼‰å…¥èˆ‡ CSV å¯«å…¥ ======
def load_parameters_from_csv(filepath):
    df = pd.read_csv(filepath, dtype=str).fillna("")
    params_list = []
    for _, row in df.iterrows():
        params_list.append({
            "case_year": row["case_year"].strip(),
            "case_type": row["case_type"].strip(),
            "case_number_start": row["case_number_start"].strip(),
            "case_number_end": row["case_number_end"].strip(),
            "case_cause": row["case_cause"].strip(),
            "case_judgement": row["case_judgement"].strip(),
            "full_text": row["full_text"].strip(),
            "case_size_min": row["case_size_min"].strip(),
            "case_size_max": row["case_size_max"].strip(),
            "start_year": row["start_year"].strip(),
            "start_month": row["start_month"].zfill(2),
            "start_day": row["start_day"].zfill(2),
            "end_year": row["end_year"].strip(),
            "end_month": row["end_month"].zfill(2),
            "end_day": row["end_day"].zfill(2),
            "case_category": [x.strip() for x in row["case_category"].split(';') if x.strip()]
        })
    return params_list

def save_structured_result(data_dict):
    file_exists = os.path.exists(CSV_RESULT_PATH)
    with open(CSV_RESULT_PATH, 'a', encoding='utf-8-sig', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=list(data_dict.keys()))
        if not file_exists:
            writer.writeheader()
        writer.writerow(data_dict)

# ====== å››ç¨®è¼¸å‡º I/Oï¼ˆæ–°å¢ï¼‰ ======
def _ensure_csv_headers(path: str, headers: list):
    need_header = not os.path.exists(path)
    f = open(path, "a", newline="", encoding="utf-8-sig")
    w = csv.DictWriter(f, fieldnames=headers)
    if need_header:
        w.writeheader()
    # å›å‚³ writer èˆ‡æª”æ¡ˆ handleï¼ˆç”±å‘¼å«ç«¯é—œé–‰ï¼Œé¿å…é »ç¹é–‹é—œæª”ï¼‰
    return w, f

def append_split_jsonl(row: dict):
    with open(JSONL_SPLIT_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

def append_full_jsonl(row: dict):
    with open(JSONL_FULL_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

def append_mgmt_csv(row: dict):
    headers = ["url","court","year","zi","number","case_type","judgment_type","cause","has_pdf","fetched_at"]
    w, fh = _ensure_csv_headers(MGMT_CSV_PATH, headers)
    try:
        w.writerow({k: row.get(k, "") for k in headers})
    finally:
        fh.close()

def append_db_csv(row: dict):
    headers = ["jud_key","url","court","year","zi","number","case_type","judgment_type","pdf_path","created_at"]
    w, fh = _ensure_csv_headers(DB_CSV_PATH, headers)
    try:
        w.writerow({k: row.get(k, "") for k in headers})
    finally:
        fh.close()

def get_first_div_by_id(soup):
    """
    ç”¨ div id (pasted_paragraph_) é–å®šç¬¬ä¸€å€‹æ®µè½
    - ä¸æ’åºï¼Œåªå– HTML åŸå§‹é †åºçš„ç¬¬ä¸€å€‹åŒ¹é…
    - è¿”å› (id, æ–‡å­—)
    """
    first_div = soup.find("div", id=re.compile(r"^pasted_paragraph_"))
    if not first_div:
        return None, None

    # å–æ–‡å­—ï¼ˆä¿®å¾© abbr æ‹†å­—èˆ‡ç©ºç™½ï¼‰
    html = str(first_div)
    html = re.sub(r'<abbr[^>]*>([^<]*)</abbr>\s*äºº', r'\1äºº', html)
    html = re.sub(r'<abbr[^>]*>([^<]*)</abbr>', r'\1', html)
    text = BeautifulSoup(html, "html.parser").get_text(" ", strip=True)
    text = re.sub(r"\s+", " ", text).strip()
    # å»é™¤ä¸­æ–‡å­—é–“å¤šé¤˜ç©ºç™½
    text = re.sub(r"(?<=[\u4e00-\u9fa5])\s+(?=[\u4e00-\u9fa5])", "", text)
    return first_div["id"], text


def clean_html_before_extract(soup):
    """ç§»é™¤é å°¾åƒåœ¾èˆ‡éæ­£æ–‡å€å¡Š"""
    junk_selectors = [
        "#qouteurl", ".btn", ".related", ".footer",
        ".lawlist", ".noprint", ".share", ".function",
        ".nav", ".breadcrumb", ".header", ".announce"
    ]
    for sel in junk_selectors:
        for junk in soup.select(sel):
            junk.decompose()
    return soup

# ====== å¡«è¡¨ ======
async def fill_query_form(page, params):
    print_debug("è¼‰å…¥æŸ¥è©¢é é¢...")
    await page.goto(BASE_URL, timeout=60000)
    await asyncio.sleep(2)

    print_debug("å‹¾é¸æ¡ˆä»¶é¡åˆ¥...")
    for v in params["case_category"]:
        cb_selector = f'input[name="jud_sys"][value="{v}"]'
        await page.check(cb_selector)
        await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    print_debug("è¼¸å…¥è£åˆ¤æœŸé–“...")
    await page.fill("#dy1", params["start_year"])
    await page.fill("#dm1", params["start_month"])
    await page.fill("#dd1", params["start_day"])
    await page.fill("#dy2", params["end_year"])
    await page.fill("#dm2", params["end_month"])
    await page.fill("#dd2", params["end_day"])
    await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    print_debug("è¼¸å…¥è£åˆ¤å­—è™Ÿèˆ‡é—œéµå­—...")
    if params["case_year"]:
        await page.fill("#jud_year", params["case_year"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_type"]:
        await page.fill("#jud_case", params["case_type"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_number_start"]:
        await page.fill("#jud_no", params["case_number_start"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_cause"]:
        await page.fill("#jud_title", params["case_cause"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_judgement"]:
        await page.fill("#jud_jmain", params["case_judgement"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["full_text"]:
        await page.fill("#jud_kw", params["full_text"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_size_min"]:
        await page.fill("#KbStart", params["case_size_min"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))
    if params["case_size_max"]:
        await page.fill("#KbEnd", params["case_size_max"]); await asyncio.sleep(random.uniform(*WAIT_BEFORE_CHECKBOX))

    await asyncio.sleep(WAIT_AFTER_FORM_INPUT)

# ====== å¤§å‡½å¼ï¼šæäº¤æŸ¥è©¢ä¸¦å–å¾—çµæœ iframeï¼ˆæ–°å¢ï¼‰ ======
async def submit_query_and_get_iframe(page, params):
    await fill_query_form(page, params)
    iframe, status = await submit_and_get_iframe(page)

    if status == "NO_DATA":
        # æ­£å¸¸æƒ…æ³ï¼šåƒæ•¸çµ„åˆçœŸçš„æ²’æœ‰è³‡æ–™ï¼Œä¸è¨˜ log
        return None, "NO_DATA"

    if iframe is None:
        # æŠ€è¡“æ€§å•é¡Œï¼šå¿…é ˆè¨˜ LOG-IFRAME
        log_stage("IFRAME", "æ­¤åƒæ•¸çµ„åˆç„¡æ³•å–å¾—çµæœ iframeï¼ˆéQ003ï¼‰", params=str(params)[:120])
        return None, "UNKNOWN"

    # ç­‰çµæœå°±ç·’
    state = await wait_results_state(iframe, max_wait_sec=20)
    if state == "NO_DATA":
        return None, "NO_DATA"
    if state == "UNKNOWN":
        # DOM è®Šå‹•/é€¾æ™‚ï¼Œç•¶æˆ 0 ç­†å›å ±ï¼ˆä¸ç•¶éŒ¯ï¼‰
        return None, "UNKNOWN"

    return iframe, "OK"

# ====== åˆ†é å¤§å‡½å¼ï¼ˆå–ä»£åŸ process_search_pagesï¼‰ ======
async def process_result_pages(page, iframe, context, params, params_pdf_dir=None):
    # â€”â€” ç¬¬ä¸€éšæ®µï¼šæˆåŠŸé€²å…¥åˆ†å±¤é ï¼ˆåˆ†å±¤é  = çµæœæ¸…å–®é ï¼‰â€”â€”
    log_stage("STAGE1", "å·²é€²å…¥çµæœæ¸…å–®é ï¼ˆåˆ†å±¤é ï¼‰")

    # å…ˆå˜—è©¦ã€Œå…± X ç­†ã€
    total_count_hint = await get_total_result_count_hint(iframe)
    final_total_count = None
    if total_count_hint is not None:
        final_total_count = total_count_hint
    else:
        # ä¼°ç®—ï¼šå–®é /æœ€å¾Œä¸€é å›æ¨
        try:
            total_pages = await get_total_pages(iframe)
            if total_pages <= 1:
                first_page_rows = await count_rows_on_current_page(iframe)
                final_total_count = first_page_rows
            else:
                # è·³æœ€å¾Œä¸€é æ•¸åˆ—
                cur_iframe = iframe
                await select_page_by_index(iframe, page, total_pages)
                await asyncio.sleep(1.0)
                cur_iframe = await get_iframe(page)
                last_rows = await count_rows_on_current_page(cur_iframe)

                # å›ç¬¬ä¸€é æ‹¿ page_size
                await select_page_by_index(cur_iframe, page, 1)
                await asyncio.sleep(1.0)
                cur_iframe = await get_iframe(page)
                page_size = await count_rows_on_current_page(cur_iframe) or 20
                final_total_count = (total_pages - 1) * page_size + last_rows

                # å›ç¬¬ä¸€é é–‹å§‹è™•ç†
                await select_page_by_index(cur_iframe, page, 1)
                await asyncio.sleep(0.8)
                iframe = await get_iframe(page)
        except Exception as e:
            log_stage("STAGE1", "ç„¡æ³•è¨ˆç®—ç¸½ç­†æ•¸ï¼ˆå°‡ä»å˜—è©¦æŠ“å–æœ¬é ï¼‰", error=str(e))

    processed_count = 0
    max_page = TEST_MODE_LIMIT_PAGE  # ä½ çš„æ¸¬è©¦ä¸Šé™

    for cur_page in range(1, max_page + 1):
        print_debug(f"==é–‹å§‹è™•ç†ç¬¬ {cur_page} é ==")

        # ç¿»é ï¼ˆç¬¬1é ä¸ç”¨ï¼‰
        if cur_page > 1:
            ok = await goto_page(page, iframe, cur_page)
            if not ok:
                break
            await asyncio.sleep(1.5)
            iframe = await get_iframe(page)
            if not iframe or "qryresultlst.aspx" not in (iframe.url or ""):
                print_debug("é é¢è·³è½‰å¾Œæ‰¾ä¸åˆ°æœ‰æ•ˆ iframeï¼Œåœæ­¢ã€‚")
                break
            # â€”â€” ç¬¬ä¸‰éšæ®µ LOGï¼šç¿»é æˆåŠŸ â€”â€”
            log_stage("STAGE3", "å‰å¾€ä¸‹ä¸€åˆ†é ", next_page=cur_page)

        # æ“·å–æœ¬é æ‰€æœ‰æ˜ç´° URLï¼ˆé€™è£¡ç›´æ¥æŠ“ elementï¼Œè®“æ˜ç´°éšæ®µå»é»ï¼‰
        table = await get_result_table(iframe)
        if not table:
            log_stage("URLS", "æŸ¥ç„¡çµæœ tableï¼ˆæœ¬é ï¼‰")
            break

        rows = await table.query_selector_all("tbody > tr")
        url_elems = []
        for r in rows:
            link = await r.query_selector("td a[href*='id='], td a[href*='ty=JD'], td a[href*='ty=JUDBOOK']")
            if link:
                url_elems.append(link)

        if not url_elems:
            log_stage("URLS", "æœªæ“·å–åˆ°ä»»ä½•æ˜ç´° URLï¼ˆè«‹æª¢æŸ¥ selector æˆ–é é¢è®Šæ›´ï¼‰")
            break
        else:
            log_stage("URLS", "å·²æ“·å–æ˜ç´° URL", count=len(url_elems))

        # â€”â€” æ˜ç´°éšæ®µï¼ˆç¬¬äºŒéšæ®µ LOG åœ¨è£¡é¢ï¼‰â€”â€”
        handled, hit_limit = await process_details(page, url_elems, context, params, params_pdf_dir)
        processed_count += handled

        # å–®é ä¸Šé™ï¼ˆä½ çš„ TEST_MODE_LIMIT_PER_PAGEï¼‰
        if hit_limit:
            log_stage("STAGE3", "é”åˆ°æœ¬åˆ†é è™•ç†ä¸Šé™", page_index=cur_page, handled=handled)

    return processed_count, (final_total_count if final_total_count is not None else processed_count)

# ====== æ˜ç´°å¤§å‡½å¼ï¼ˆæ–°å¢ï¼‰ ======
async def process_details(page, url_elements, context, params, params_pdf_dir=None):
    """
    å›å‚³ï¼š(æœ¬é è™•ç†æ•¸, æ˜¯å¦é”ä¸Šé™)
    - é€ç­†é–‹å•Ÿæ˜ç´° â†’ æ“·å–è³‡æ–™ â†’ ç”¢å‡ºæª”æ¡ˆï¼ˆJSONL / PDF or PNG / æ¥­ç®¡CSV / DB CSVï¼‰
    - ç”¢å‡ºæª”æ¡ˆå‰è¨˜éŒ„ LOG-STAGE2
    """
    base_dir = params_pdf_dir or PDF_SNAPSHOT_PATH
    os.makedirs(base_dir, exist_ok=True)

    handled = 0
    for idx, link in enumerate(url_elements, start=1):
        if idx > TEST_MODE_LIMIT_PER_PAGE:
            return handled, True

        try:
            # å˜—è©¦ç”¨æ–°é é–‹å•Ÿï¼Œå®¹å¿åŒé å°èˆª
            async with context.expect_page() as popup_info:
                try:
                    await link.evaluate("a => a.target = '_blank'")
                except:
                    pass
                await link.click(button="middle")
            try:
                popup = await popup_info.value
                page_like = popup
            except:
                popup = None
                page_like = page

            await page_like.bring_to_front()
            await page_like.wait_for_load_state("domcontentloaded", timeout=20000)
            await asyncio.sleep(random.uniform(*WAIT_DETAIL_CONTENT))
            await human_noise(page_like)

            # ====== åŸºæœ¬å‘½åèˆ‡åˆ†äº«é€£çµ ======
            share_link = clean_url(page_like.url)
            html = await page_like.content()
            soup = BeautifulSoup(html, "html.parser")

            # å–ã€Œè£åˆ¤å­—è™Ÿï¼è£åˆ¤æ—¥æœŸï¼è£åˆ¤æ¡ˆç”±ã€
            def get_value_by_label_bs(soup, label):
                for th, td in zip(soup.select("div.col-th"), soup.select("div.col-td")):
                    th_text = th.get_text(strip=True)
                    if label in th_text:
                        return td.get_text(strip=True)
                return ""

            title_text = get_value_by_label_bs(soup, "è£åˆ¤å­—è™Ÿ")
            date_text_raw = get_value_by_label_bs(soup, "è£åˆ¤æ—¥æœŸ")
            cause_text = get_value_by_label_bs(soup, "è£åˆ¤æ¡ˆç”±")

            # è¨˜éŒ„åˆ†äº«æ¸…å–®
            date_roc = date_text_raw or ""
            date_str = re.sub(r"æ°‘åœ‹\s*(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥",
                              lambda m: f"{int(m.group(1))}/{int(m.group(2)):02}/{int(m.group(3)):02}", date_roc)
            share_title = f"{date_str}_{(title_text or '').replace(' ', '')}"
            save_share_url(share_title, share_link or "")

            # è§£ææ°‘åœ‹æ—¥æœŸ â†’ yyy/mm/ddï¼ˆæª”åç”¨ï¼‰
            date_text = re.sub(
                r"æ°‘åœ‹\s*(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥",
                lambda m: f"{int(m.group(1))}/{int(m.group(2)):02}/{int(m.group(3)):02}",
                date_text_raw or ""
            )

            # ç°¡åŒ–æª”åï¼ˆç”¨è£åˆ¤å­—è™Ÿèˆ‡æ—¥æœŸï¼‰
            base_name = sanitize_filename((date_text or "date") + "_" + (title_text or "judgment"))
            filename_base = os.path.join(base_dir, base_name)
            pdf_path = filename_base + ".pdf"
            png_path = filename_base + ".png"

            # ====== è§’è‰²/æ¬„ä½è§£æï¼ˆæ²¿ç”¨ä½ åŸæœ¬é‚è¼¯ï¼‰ ======

            def extract_parties_from_abbr(soup, debug=False):
                role_set = set(PARTY_ROLE_MARKERS)
                result = {r: [] for r in PARTY_ROLE_MARKERS}

                paragraphs = soup.find_all("div", id=re.compile(r"^(pasted_|new_)?paragraph_"))
                merged_lines = []
                buffer = ""
                prev_text = ""

                for idx, div in enumerate(paragraphs):
                    # Step 1ï¸ ä¿®å¾© abbr çµæ§‹
                    html = str(div)
                    html = re.sub(r'<abbr[^>]*>([^<]*)</abbr>\s*äºº', r'\1äºº', html)
                    html = re.sub(r'<abbr[^>]*>([^<]*)</abbr>\s*å¾‹å¸«', r'\1å¾‹å¸«', html)
                    html = re.sub(r'<abbr[^>]*>([^<]*)</abbr>', r'\1', html)
                    html = html.replace("&nbsp;", " ")

                    # Step 2ï¸ è½‰ç‚ºä¹¾æ·¨æ–‡å­— + ä¿®æ­£ä¸­æ–‡å­—é–“ç©ºç™½
                    text = BeautifulSoup(html, "html.parser").get_text(" ", strip=True)
                    text = re.sub(r"\s+", " ", text).strip()
                    text = re.sub(r"(?<=[\u4e00-\u9fa5])\s+(?=[\u4e00-\u9fa5])", "", text)
                    if not text:
                        continue

                    # Step 2.1ï¸ æª¢æŸ¥æ˜¯å¦ç‚ºè·¨è¡Œè§’è‰²ï¼ˆä¾‹ï¼šè²è«‹äºº / å³ å‚µå‹™äººï¼‰
                    if idx + 1 < len(paragraphs):
                        next_html = str(paragraphs[idx + 1])
                        next_text = BeautifulSoup(next_html, "html.parser").get_text(" ", strip=True)
                        next_text = re.sub(r"\s+", " ", next_text).strip()
                        next_text = re.sub(r"(?<=[\u4e00-\u9fa5])\s+(?=[\u4e00-\u9fa5])", "", next_text)
                        if re.search(r"(äºº|å¾‹å¸«)$", text) and next_text.startswith("å³"):
                            text = text + " " + next_text
                            if debug:
                                print(f"  â†³ [è·¨è¡Œè§’è‰²å»¶ä¼¸] {text}")

                    if debug:
                        print(f"[DEBUG] è¡Œ {idx+1}: {text}")

                    # Step 2.2 è‹¥ä¸Šä¸€è¡Œä»¥ã€Œå³ã€çµå°¾ä¸”æœ¬è¡Œæœ‰è§’è‰² â†’ åˆä½µ
                    if buffer and re.search(r"å³\s*$", buffer) and any(role in text[:8] for role in role_set):
                        if debug:
                            print(f"  â†³ [MERGE å³çµæ§‹] {buffer[-10:]} + {text[:15]}")
                        buffer = buffer.rstrip("å³").strip() + " å³ " + text
                        prev_text = text
                        continue

                    # Step 2.3 è‹¥æœ¬è¡Œä»¥ã€Œå³ã€é–‹é ­ï¼ˆåŒ…å« abbr çµæ§‹ï¼‰ â†’ åˆä½µåˆ°ä¸Šä¸€è¡Œ
                    if text.strip().startswith("å³") or re.match(r"^å³\s*<abbr", html):
                            # è‹¥ä¸Šä¸€è¡Œ buffer å·²ç¶“å«æœ‰æœ¬è¡Œå…§å®¹ â†’ è·³éï¼ˆé˜²é‡è¤‡ï¼‰
                        if buffer and text in buffer:
                            if debug:
                                print(f"  â†³ [è·³éé‡è¤‡å³è¡Œ] {text[:20]}")
                            continue


                        # å˜—è©¦ä¿®å¾© abbr ä¸­æ‹†é–‹çš„è§’è‰²è©
                        fixed_text = re.sub(r'<abbr[^>]*>([^<]*)</abbr>\s*äºº', r'\1äºº', html)
                        fixed_text = re.sub(r'<abbr[^>]*>([^<]*)</abbr>', r'\1', fixed_text)
                        fixed_text = BeautifulSoup(fixed_text, "html.parser").get_text(" ", strip=True)
                        fixed_text = re.sub(r"\s+", " ", fixed_text)
                        fixed_text = re.sub(r"(?<=[\u4e00-\u9fa5])\s+(?=[\u4e00-\u9fa5])", "", fixed_text)

                        # è‹¥è£¡é¢æœ‰è§’è‰²é—œéµå­— â†’ åˆä½µ
                        if buffer:
                            if debug:
                                print(f"  â†³ [MERGE å³çµæ§‹-ä¸‹å³ä¸Šè§’] {buffer[-10:]} + {fixed_text[:20]}")
                            buffer = buffer.strip() + " " + fixed_text
                            prev_text = fixed_text
                            continue


                    # Step 3ï¸ åˆ¤æ–·ä¸»æ–‡ or è§’è‰²è¡Œ
                    if any(stop in text for stop in judgment_start_markers):
                        if buffer:
                            merged_lines.append(buffer.strip())
                            buffer = ""
                        if debug:
                            print("  â†³ [ä¸»æ–‡åµæ¸¬] åµæ¸¬åˆ°æ­£æ–‡é–‹é ­è© â†’ çµæŸè§’è‰²æ®µ")
                        break

                    has_role = any(role in text for role in role_set)
                    is_new_role = False
                    is_continuation = False

                    if has_role and re.match(r"^.{0,5}(" + "|".join(role_set) + ")", text):
                        is_new_role = True

                    if not is_new_role:
                        if re.match(r"^[ã€,ï¼ŒåŠèˆ‡æˆ–åŒå¦ï¼ˆ(]", text):
                            is_continuation = True
                        elif re.search(r"(è‚¡ä»½æœ‰é™å…¬å¸|å…¬å¸|äº‹å‹™æ‰€|å¤§å­¸|é†«é™¢)$", text):
                            is_continuation = True
                        elif re.match(r"^[\u4e00-\u9fa5]{2,4}$", text):
                            is_continuation = True
                        elif not any(stop in text for stop in judgment_start_markers):
                            is_continuation = True

                    if is_new_role:
                        if buffer:
                            merged_lines.append(buffer.strip())
                        buffer = text
                        if debug:
                            print("  â†³ [æ–°è§’è‰²è¡Œ] é–‹å§‹æ–°çš„è§’è‰²æ®µ")
                    elif is_continuation:
                        buffer += " " + text
                        if debug:
                            print("  â†³ [å»¶çºŒè¡Œ] åˆä½µé€²ä¸Šä¸€è§’è‰²æ®µ")
                    else:
                        if buffer:
                            merged_lines.append(buffer.strip())
                            buffer = ""
                        if debug:
                            print("  â†³ [éè§’è‰²è¡Œ] çµæŸè§’è‰²æ®µ")
                    prev_text = text

                if buffer:
                    merged_lines.append(buffer.strip())

                if debug:
                    print("\n[DEBUG] åˆä½µå¾Œè§’è‰²å€æ®µï¼š")
                    for i, seg in enumerate(merged_lines, 1):
                        print(f"  {i}. {seg}")

                # Step 5ï¸ è§’è‰²å…§å®¹è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                for line in merged_lines:
                    # åŒè¡Œå¯æœ‰å¤šè§’è‰²ï¼Œé•·è©å„ªå…ˆ
                    matched_roles = []
                    for r in sorted(role_set, key=len, reverse=True):
                        if r not in line:
                            continue
                        # è‹¥æœ¬è¡ŒåŒæ™‚åŒ…å«ã€Œæ³•å®šä»£ç†äººã€ä¸” r æ˜¯ã€Œä»£ç†äººã€ï¼Œå‰‡è·³éï¼Œé¿å…é‡ç–Š
                        if r == "ä»£ç†äºº" and any(big in line for big in ["æ³•å®šä»£ç†äºº", "è¨´è¨Ÿä»£ç†äºº", "é¸ä»»ä»£ç†äºº"]):
                            continue
                        matched_roles.append(r)
                    if not matched_roles:
                        continue

                    for matched_role in matched_roles:
                        after = line.split(matched_role, 1)[-1].strip()
                        after = re.sub(r"[\(\)ï¼ˆï¼‰\[\]]", "", after)

                        # è‹¥ä»å«ä¸»æ–‡è© â†’ æˆªæ–·
                        for stop in judgment_start_markers:
                            if stop in after:
                                after = after.split(stop)[0].strip()

                        if len(after) < 1 or len(after) > 150:
                            continue

                        # ã€Œå³ã€é‚è¼¯ï¼šA(å³B) å…©è§’è‰²å…±ç”¨åŒä¸€å…§å®¹
                        m = re.search(r"å³\s*([^\s,ï¼Œã€]{1,6}?äºº)", after)
                        if m:
                            alias_role = m.group(1)
                            if alias_role in role_set:
                                cleaned = re.sub(r"å³\s*" + re.escape(alias_role), "", after)
                                cleaned = cleaned.strip(" ,;ï¼›ã€")

                                # âš ï¸ é¿å…é‡è¤‡å¯«å…¥
                                if cleaned and cleaned not in result[matched_role]:
                                    result[matched_role].append(cleaned)
                                if cleaned and cleaned not in result[alias_role]:
                                    result[alias_role].append(cleaned)

                                if debug:
                                    print(f"  â†³ [å³çµæ§‹é›™è§’è‰²] {matched_role}, {alias_role}: {cleaned}")
                                continue

                        after = re.sub(r"\s+", " ", after).strip(" ,;ï¼›ã€")
                        result[matched_role].append(after)
                        if debug:
                            print(f"  â†³ [è§’è‰²æŠ½å–] {matched_role}: {after}")

                # Step 6 æ¸…ç†é‡è¤‡è¼¸å‡º
                merged = {}
                for role, vals in result.items():
                    seen, clean = set(), []
                    for v in vals:
                        v = re.sub(r"\s+", " ", v).strip(" ,;ï¼›ã€")
                        if v and v not in seen:
                            seen.add(v)
                            clean.append(v)
                    merged[role] = ", ".join(clean)

                if debug:
                    print("\n[DEBUG] æœ€çµ‚çµæ§‹åŒ–çµæœï¼š")
                    for k, v in merged.items():
                        if v:
                            print(f"  {k}: {v}")

                return merged


            # === å‘¼å«æµç¨‹ ===
            soup = clean_html_before_extract(soup)
            first_id, first_text = get_first_div_by_id(soup)
            print("ğŸ”¹ç¬¬ä¸€å€‹ div:", first_id)
            print("ğŸ”¹æ–‡å­—å…§å®¹:", first_text)
            party_data = extract_parties_from_abbr(soup, debug=True)

            # è£åˆ¤å­—è™Ÿç´°æ‹†ï¼ˆæ²¿ç”¨ä½ çš„ regexï¼‰
            pattern = (r'(?P<æ³•é™¢>.+?)\s*(?P<å¹´åº¦>\d{2,3})\s*å¹´åº¦(?P<å­—>\S+?)å­—ç¬¬\s*(?P<å­—è™Ÿ>\d+)\s*è™Ÿ(?P<åˆ‘æ°‘>(æ°‘äº‹|åˆ‘äº‹))?\s*(?P<è£å®šé¡å‹>è£å®š|åˆ¤æ±º|è£åˆ¤|å‘½ä»¤|è£åˆ¤|å…¶ä»–)?')
            match = re.match(pattern, title_text or "")
            court = match.group("æ³•é™¢") if match else ""
            year = match.group("å¹´åº¦") if match else ""
            case_word = match.group("å­—") if match else ""
            case_number = match.group("å­—è™Ÿ") if match else ""
            case_type = match.group("åˆ‘æ°‘") if match and match.group("åˆ‘æ°‘") else ""
            judgment_type = match.group("è£å®šé¡å‹") if match and match.group("è£å®šé¡å‹") else ""

            # å˜—è©¦æŠ“å–ã€Œæ­£æ–‡æ–‡å­—ã€ä¾› JSONLï¼ˆè‹¥æŠ“ä¸åˆ°å°±ä»¥æ‘˜è¦ç‚ºä¸»ï¼‰
            body_text = ""
            main_candidates = ["#jud-content",".jud","div.card-block","div#content"]
            for sel in main_candidates:
                node = soup.select_one(sel)
                if node:
                    body_text = node.get_text("\n", strip=True)
                    break
            if not body_text:
                body_text = soup.get_text("\n", strip=True)[:15000]  # é˜²å¤ªé•·

            # ====== ç”¢ç”Ÿçµæ§‹åŒ–æ®µè½ ======
            structured_json = build_structured_json(body_text)

            # â€”â€” ç¬¬äºŒéšæ®µï¼šç”¢å‡ºæª”æ¡ˆå‰ LOG â€”â€”
            log_stage("STAGE2", "å³å°‡ç”¢å‡ºæª”æ¡ˆï¼ˆJSON/PDF/æ¥­ç®¡CSV/DBCSVï¼‰", url=share_link)

            fetched_at = now_tpe_str()

            # ====== ä¸‹è¼‰ PDFï¼ˆç«™å…§è½‰å­˜ï¼Œå¤±æ•—å‰‡æ‰“å° PDFï¼‰ï¼‹ PNG ======
            if OUTPUT_OPTIONS.get("PDF" , False):
              try:
                if not (os.path.exists(pdf_path) and os.path.exists(png_path)):
                    try:
                        await download_via_site_pdf(page_like, pdf_path)
                        print_debug(f"å·²ç”¨ã€ç«™å…§è½‰å­˜PDFã€ä¸‹è¼‰ï¼š{pdf_path}")
                    except Exception as e_download:
                        print_debug(f"ç«™å…§è½‰å­˜PDFå¤±æ•—ï¼Œæ”¹ç”¨æ‰“å°PDFå‚™æ´ï¼š{e_download}")
                        await save_pdf_snapshot(page_like, pdf_path)
                        print_debug(f"å·²å­˜PDF(å‚™æ´)ï¼š{pdf_path}")
                    await save_png_snapshot(page_like, png_path)
                    print_debug(f"å·²æˆªåœ–ï¼š{png_path}")

              except Exception as e:
                log_stage("PDFPNG", "PDF/PNG è¼¸å‡ºå¤±æ•—", error=str(e))
                print_debug(f"PDF/PNG è¼¸å‡ºå¤±æ•—ï¼š{e}")

            # 1-1) FULL_JSONLï¼ˆå…¨æ–‡/æ‘˜è¦ï¼‰
            if OUTPUT_OPTIONS.get("FULL_JSONL" , False):
              append_full_jsonl({
                  "url": share_link,
                  "body_text": body_text
              })
              print_debug(f"å·²å„²å­˜FULL_JSONLï¼š{JSONL_FULL_PATH}")


            # 1-2) SPLIT_JSONLï¼ˆå…¨æ–‡/æ‘˜è¦ï¼‰
            if OUTPUT_OPTIONS.get("SPLIT_JSONL" , False):
              append_split_jsonl({
                  "url": share_link,
                  "fetched_at": fetched_at,
                  "title": title_text,
                  "date": date_text,
                  "cause": cause_text,
                  **{r: party_data.get(r, "") for r in PARTY_ROLE_MARKERS},  # å®‰å…¨å±•é–‹è£œç©º
                  "court": court,
                  "year": year,
                  "zi": case_word,
                  "number": case_number,
                  "case_type": case_type,
                  "judgment_type": judgment_type,
                  "pdf_path": pdf_path,
                  "png_path": png_path,
                  "structured_content": structured_json
              })
              print_debug(f"å·²å„²å­˜SPLIT_JSONLï¼š{JSONL_SPLIT_PATH}")
            # 2) æ¥­ç®¡ CSVï¼ˆç²¾ç°¡æ‘˜è¦ï¼‰
            if OUTPUT_OPTIONS.get("MGMT_CSV" , False):
              append_mgmt_csv({
                  "url": share_link,
                  "court": court,
                  "year": year,
                  "zi": case_word,
                  "number": case_number,
                  "case_type": case_type,
                  "judgment_type": judgment_type,
                  "cause": cause_text,
                  "has_pdf": os.path.exists(pdf_path),
                  "fetched_at": fetched_at
              })
              print_debug(f"å·²å„²å­˜MGMT_CSVï¼š{MGMT_CSV_PATH}")

            # 3) è³‡æ–™åº« CSVï¼ˆæ‰å¹³ã€å¾ŒçºŒå¯æ­£è¦åŒ–ï¼‰
            if OUTPUT_OPTIONS.get("DB_CSV" , False):
              jud_key = f"{court}-{year}-{case_word}-{case_number}".strip("-")
              append_db_csv({
                  "jud_key": jud_key,
                  "url": share_link,
                  "court": court,
                  "year": year,
                  "zi": case_word,
                  "number": case_number,
                  "case_type": case_type,
                  "judgment_type": judgment_type,
                  "pdf_path": pdf_path,
                  "created_at": fetched_at
              })
              print_debug(f"å·²å„²å­˜DB_CSVï¼š{DB_CSV_PATH}")

            # 4) ä½ åŸæœ¬çš„ã€Œçµæ§‹åŒ– CSVã€ï¼ˆä¿ç•™ï¼‰
            if OUTPUT_OPTIONS.get("STRUCTURED_CSV" , False):
              result = {
                  "è£åˆ¤å­—è™Ÿ": title_text,
                  "è£åˆ¤å­—è™Ÿ_æ³•é™¢": court,
                  "è£åˆ¤å­—è™Ÿ_å¹´åº¦": year,
                  "è£åˆ¤å­—è™Ÿ_å­—": case_word,
                  "è£åˆ¤å­—è™Ÿ_å­—è™Ÿ": case_number,
                  "è£åˆ¤å­—è™Ÿ_åˆ‘äº‹æ°‘äº‹": case_type,
                  "è£åˆ¤å­—è™Ÿ_è£å®š": judgment_type,
                  "è£åˆ¤æ—¥æœŸ": date_text,
                  "è£åˆ¤æ¡ˆç”±": cause_text,
                  **{r: party_data.get(r, "") for r in PARTY_ROLE_MARKERS},  # å®‰å…¨å±•é–‹è£œç©º
                  "æ˜ç´°ç¶²å€": share_link,
              }
              save_structured_result(result)
              print_debug(f"å·²å„²å­˜STRUCTURED_CSVï¼š{CSV_RESULT_PATH}")

            # é˜²çˆ¬æš«åœ
            if await is_busy_page(page_like):
                print_debug("!!! åµæ¸¬åˆ°é˜²çˆ¬èŸ²æµé‡é™åˆ¶ï¼Œæš«åœ 5 åˆ†é˜å†ç¹¼çºŒ...")
                if popup:
                    await page_like.close()
                await asyncio.sleep(WAIT_BUSY_PAGE)

            if popup:
                await page_like.close()

            handled += 1

        except Exception as e:
            # â€”â€” DETAIL å¤±æ•—å›å ± â€”â€”
            log_stage("DETAIL", "æ˜ç´°è™•ç†å¤±æ•—", error=str(e))
            try:
                if 'page_like' in locals() and page_like:
                    err_base = os.path.join(base_dir, f"error_{idx}")
                    await save_png_snapshot(page_like, err_base + ".png")
                    await save_pdf_snapshot(page_like, err_base + ".pdf")
                    try:
                        await page_like.close()
                    except:
                        pass
                await asyncio.sleep(WAIT_AFTER_ERROR)
            except:
                pass
            continue

    return handled, False

# ====== å–®çµ„åƒæ•¸æµç¨‹ï¼ˆæ•´åˆæ–°çš„å€å¡Šï¼‰ ======
async def run_scraper_with_params(params):
    async with async_playwright() as p:
        # ä¸‹è¼‰ä¸€å®šè¦é–‹ accept_downloads
        browser = await p.chromium.launch(headless=True, args=['--no-sandbox', "--disable-dev-shm-usage"])
        context = await browser.new_context(accept_downloads=True)
        page = await context.new_page()

        # â€”â€” ç€è¦½å™¨å•Ÿå‹•å›å ± â€”â€”
        log_stage("BOOT", "ç€è¦½å™¨å•Ÿå‹•æˆåŠŸ")

        try:
            # [ADD] ç‚ºé€™ä¸€çµ„åƒæ•¸å»ºç«‹å°ˆå±¬ PDF ç›®éŒ„
            params_pdf_dir = build_params_pdf_dir(params)

            # æŸ¥è©¢ â†’ å–å¾— iframeï¼ˆå« Q003ã€UNKNOWN è™•ç†ï¼‰
            iframe, status = await submit_query_and_get_iframe(page, params)

            # å¦‚æœé é¢ç›´æ¥å‘ˆç¾NO_DATAæˆ–æ˜¯ç„¡iframeï¼Œå‰‡ç›´æ¥å°‡åƒæ•¸çµ„åˆç´€éŒ„è‡³query_log.txt
            if status == "NO_DATA":
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return
            if status != "OK" or not iframe:
                # IFRAME æŠ€è¡“æ€§å¤±æ•—å·²åœ¨ submit_query_and_get_iframe() è¢« LOG-IFRAME è¨˜éŒ„
                append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
                return

            # åˆ†é  + æ˜ç´°ï¼ˆå«ä¸‰éšæ®µ LOGï¼‰
            processed_count, final_total_count = await process_result_pages(page, iframe, context, params, params_pdf_dir)

            # å¯«å…¥æŸ¥è©¢å½™ç¸½ç´€éŒ„
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), final_total_count)

        # éŒ¯èª¤ä¾‹å¤–
        except PlaywrightTimeoutError as e:
            print_debug(f"Timeoutç™¼ç”Ÿ: {e}")
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
        except Exception as e:
            print_debug("ä¸»æµç¨‹å‡ºç¾éŒ¯èª¤: " + traceback.format_exc())
            append_query_log(params.get("full_text",""), params.get("case_cause",""), now_tpe_str(), 0)
        finally:
            print_debug("ç¨‹å¼çµæŸ")
            if browser:
                await browser.close()

# ====== æ‰¹æ¬¡æµç¨‹ ======
async def run_all_tasks():
    """
    å¾ CSV æª”æ¡ˆä¸­è®€å–æ‰€æœ‰æŸ¥è©¢åƒæ•¸çµ„ï¼Œä¾åºå‘¼å«çˆ¬èŸ²ä¸»æµç¨‹ã€‚
    """
    # ä¸€æ¬¡æ€§åˆå§‹åŒ–ï¼ˆä¸å†æ¯çµ„æ¸…ç©ºï¼‰
    ensure_headers()

    # åƒæ•¸æª”è·¯å¾‘æ›´ç©©å¥ï¼šå„ªå…ˆ params(name).csvï¼Œæ‰¾ä¸åˆ°ç”¨ params.csv
    preferred = os.path.join(SAVE_DIR, "params(name).csv")
    fallback = os.path.join(SAVE_DIR, "params.csv")
    if os.path.exists(preferred):
        csv_path = preferred
    elif os.path.exists(fallback):
        csv_path = fallback
    else:
        print("âš ï¸ éŒ¯èª¤ï¼šæœªæ‰¾åˆ° params(name).csv æˆ– params.csvï¼Œè«‹ä¸Šå‚³å¾Œé‡æ–°åŸ·è¡Œã€‚")
        from google.colab import files
        files.upload()
        if os.path.exists(preferred):
            csv_path = preferred
        elif os.path.exists(fallback):
            csv_path = fallback
        else:
            print("âŒ ä¸Šå‚³å¤±æ•—æˆ–ä»æœªæ‰¾åˆ°æª”æ¡ˆï¼Œç¨‹å¼çµ‚æ­¢ã€‚")
            return

    params_list = load_parameters_from_csv(csv_path)
    for i, params in enumerate(params_list, 1):
        print_debug(f"========== é–‹å§‹ç¬¬ {i} çµ„åƒæ•¸ ==========")
        await run_scraper_with_params(params)

# ====== å…¥å£ ======
if __name__ == "__main__":
    await run_all_tasks()
